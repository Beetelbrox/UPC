---
output:
  pdf_document: default
  html_document: default
---
 ---
title: "Lab 04 - Non-linear regression on dependency trees"
author: "Francisco Javier Jurado, Roger Pujol Torramorell"
date: "October 29, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Load and install necessary packages
requiredPackages <- c("knitr", "rstudioapi", "kableExtra", "stats")

for (pac in requiredPackages) {
    if(!require(pac,  character.only=TRUE)){
        install.packages(pac, repos="http://cran.rstudio.com")
        library(pac,  character.only=TRUE)
    }
}
rm(pac)
rm(requiredPackages)

# set pwd to current directory, must load rstudioapi before. Need to check availability of API to avoid issues when knitting
if (rstudioapi::isAvailable()) setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

```

```{r, echo=FALSE}
# Model Class definition
model <- setRefClass("model", fields=list(label="character", params="ANY", f="function", formula="ANY", log_formula="ANY"))

# List of models. Each model is an object. f returns the model function given its parameters to be able to generate points
model_ensemble <- list(
  model(label="0",
        params=c(),
        f=function() { function(n) n/3 + 1/3 },
        formula=NA,
        log_formula=NA),

  model(label="1",
        params=c('b'),
        f=function(b) { function(n) (n/2)^b },
        formula=d~(n/2)^b,
        log_formula=function(d0) { log(d)~log(n/2) + 0 } ),

  model(label="2",
        params=c('a', 'b'),
        f=function(a, b) { function(n) a*n^b },
        formula=d~a*n^b,
        log_formula=function(d0) { log(d)~log(n) } ),

  model(label="3",
        params=c('a','c'),
        f=function(a, c) { function(n) a*exp(c*n) },
        formula=d~a*exp(c*n),
        log_formula=log(d)~n),

  model(label="4",
        params=c('a'),
        f=function(a) { function(n) a*log(n) },
        formula=d~a*log(n),
        log_formula=log(d)~1*log(log(n))),

  model(label="1+",
        params=c('b', 'd'),
        f=function(b, d) { function(n) (n/2)^b + d},
        formula=d~(n/2)^b + d,
        log_formula=function(d0) { log(d-d0)~log(n/2) + 0 } )
)
```

```{r, echo=FALSE}
generate_nlms <- function(seqs, ensemble) {
  lapply( seqs, function(seq) {         # Outer loop to iterate over the dependency sequences
    lapply(ensemble, function(model) {  # Inner loop to iterate over the models
      if (length(model$params) > 0) {
        lm <- lm(model$log_formula, seq)
        initial_values <- structure(lapply(1:length(model$params), function(i)
          if (model$params[i] == "a") exp(coef(lm)[[i]]) else coef(lm)[[i]] ), names=model$params)
        nls( formula=model$formula, data=seq, start=initial_values, trace=FALSE)
      } else NA
    })
  })
}

calculate_deviances <- function(seqs, models) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { deviance(models[[i]][[j]]) }
      else { sum( (seqs[[i]]$d - (seqs[[i]]$n+1)/3)^2) }
    })
  })
}

calculate_AICs <- function(seqs, models, devs) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { AIC(models[[i]][[j]]) }
      else {
        n <- length(seqs[[i]]$n)
        n*log(2*pi) + n*log(devs[[i]][[j]]/n) + n + 2
      }
    })
  })
}

# This pushes tables to the global context
generate_models_and_measures <- function (seqs, ensemble) {
  nlms <<- generate_nlms(seqs=seqs, ensemble=ensemble)
  deviances <<- calculate_deviances(seqs, nlms)
  AICs <<- calculate_AICs(seqs, nlms, deviances)
}

plot_grid <- function(data, labels, xlab, ylab, color="blue", lines=list(), line_colors=c(), log=FALSE) {
  old.par <- par(mfrow=c(ceiling(length(data)/3), 3))
  par(mar=c(4,4,2,2))
  for (i in 1:length(data)) {
    plot(x=data[[i]]$n,
         y=data[[i]]$d,
         main=labels[[i]],
         log= if(log) "xy" else '',
         xlab=xlab,
         ylab=ylab,
         col=color)
    for (j in seq_along(lines)) {
        lines(x=lines[[j]][[i]]$n,
        y=lines[[j]][[i]]$d,
        col=line_colors[[j]])
    }
  }
}
```


```{r, echo=FALSE,include=FALSE}
# Possible solutions for heteroscedasticity: work on smoothed versions of the data: eg. average all points for a value x and work on that

# plot everything to be plotted
# Check soundness by generating perfect synthetic data and checking the fit against it. R doesnt like perfect function, some error must be added to the actual data

# replace d with a z score transformation of d
# This transform could help to reduce noise in te data

```

```{r, echo=FALSE}

###########################################################################################
# Reading the data from file
###########################################################################################
  # Read the list of languages from file
filename_suffix <- "_dependency_tree_metrics.txt"
language_list <- as.vector(read.table("data/language_list.txt", header = FALSE)$V1)

# Push the in-degree sequences and theh list of entries to the global scope
dep_tree_cols <- c('n', 'k^2', 'd')
metric_seqs <- structure(lapply(language_list, function(x) data.frame(read.table(paste("data/", x, filename_suffix, sep=''), header = FALSE, col.names=dep_tree_cols))),
                         names=language_list)

avg_seqs <- lapply(metric_seqs, function(x) aggregate(x, list(x$n), mean))

###########################################################################################
# Generation of the testing sequences
###########################################################################################

# Hard-coded list of parameters and number of observations
test_params <- list(c(), c(b=0.5), c(a=0.8, b=0.5), c(a=0.8, c=0.01), c(a=0.8))
num_test_observations = 200
# Generate the functions with the provided parameters
test_functions <- lapply(1:length(test_params), function(i) do.call(model_ensemble[[i]]$f, as.list(test_params[[i]])))
# Generate the test sequences of length num_test_observations. To avoid issues with the optimizer we add some gaussian error
test_seqs <- lapply(test_functions, function(f) data.frame(
  n=1:num_test_observations,
  d=sapply(1:num_test_observations, function(i) f(i)) + rnorm(num_test_observations, sd=0.05)))
```

# Results

## On input validity [ WIP ]
```{r, echo=FALSE}
valid_k.2 <- lapply(metric_seqs, function(x) (4-(6/x$n) <= x$k.2) & (x$k.2 <= x$n-1))
valid_d <- lapply(metric_seqs, function(x) (x$k.2*x$n/(8*(x$n-1)) + 1/2 <= x$d) & (x$d <= x$n-1))
```
## Properties summary
The first table in this results section summarizes the properties of the degree sequences, in particular the sample size and the mean and standard deviation of both the number of vertices $n$ and the mean dependency length $d$:
```{r, echo=FALSE}
summary_cols <- c('N', 'mean_n', 'sd_n', 'mean_d', 'sd_d')
metric_stats <- lapply(metric_seqs, function(x) list(
                    N=length(x$n),
                    mean_n=mean(x$n),
                    sd_n=sd(x$n),
                    mean_d=mean(x$d),
                    sd_d=sd(x$d)))

summary_df <- data.frame(do.call(rbind, lapply(metric_stats, function(x) c(x$N, x$mean_n, x$sd_n, x$mean_d, x$sd_d))), row.names=language_list)
kable(summary_df, col.names=summary_cols, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```
TODO: COMMENT THE TABLE

\newpage
## Preliminary visualization
Before going any further it is a good practice to check what the data looks like so let's plot the mean depencency length $d$ vs the number of vertices $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, '# vertices', 'mean dependency length')
```
\newpage
We would like to check for any possible power-law dependencies and we can do so by plotting data taking logs on both axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(mean dependency length)', log=TRUE)
```
Note that rather than applying the log to the data itself we have set the plot axes to logarithmic, distributing the ticks in a log fashion. We can now observe that the plots suggest a power-law despite the large amount of dispersion.

\newpage
## Checking for heteroscedasticity
We have seen in the preliminary plots that there is a One of the assumptions of non-linear regression is homoscedasticity, so before generating our models we need to make sure it holds.

```{r, echo=FALSE, fig.width=16, fig.height=18}
# Careful, this contains
var_seqs <- lapply(1:length(metric_seqs), function(i) data.frame(n=avg_seqs[[i]]$n, d=sapply(avg_seqs[[i]]$n, function(x) var( metric_seqs[[i]][metric_seqs[[i]]$n == x,]$d) ) ))
plot_grid(var_seqs, language_list, '# vertices', 'mean dependency length variance')
```

Note that in order to calculate the variance for a given value of $n$ we need more than one data point so those $n$ with a single point have been omitted.

\newpage
A way to deal with this dispersion and get a clearer intuition on the underlying trend is to average the mean length for a given number of vertices:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, '# vertices', 'avg mean dependency length')
```
Although there is stil a significant amount of dispersion for larger values of $n$, we have a way clearer view of the distribution shape. By plotting the same averaged points on log-log axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', log=TRUE)
```
The data points form an almost straight line in the log-log plot (again with dispersion when $n$ gets large) so we have reasonable evidence to believe they follow a power-law distribution.  

We now want to compare how far the real scaling of $d$ is from the one existing at a random linear arrangement. For that purpose we can compare the points to the averaged ones and the expected mean length, given by $E[\langle d \rangle] = (n+1)/3$. Plotting that in a regular and double logarithmic scale:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue",
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue", log=TRUE,
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

## On correctness

```{r, echo=FALSE}
generate_models_and_measures(test_seqs, model_ensemble)
fitted_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else c()))
best_models <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(nlms), function(i) fitted_params[[i]][[ as.numeric(best_models[i]) ]])
```

Before trying to select the best model for each real dependency length sequence we would like to make sure our model selection pipeline works properly. In order to test it we generated test sequences for each one of the models in our ensemble following the actual function of each model. With regards to the parameters of the models we chose the following values (no particular criteria, we made an educated guess based on the values obtained when fitting on the real data): $a=0.8$, $b=0.5$, $c=0.01$.  
Because the optimizer has some difficulties trying to fit data that perfectly matches the function being fitted (reaches max number of iterations before convergence due to precision issues) we have added some gaussian noise as suggested in the lab session. We wanted keep the data as close as possible to the real functions so after trying several parameters we chose a $mean=0$ and $sd=0.05$ as the noise's gausian distribution parameters. By plotting the synthetic data (with linear axes) we can see the shapes of the functions in our ensemble:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(test_seqs, lapply(model_ensemble, function(x) paste("Model ", x$label)), xlab='n', ylab='f(n)')
```

Now that we have our testing data we can fit the models and calculate the AIC for each one of them:
```{r, echo=FALSE}
kable(data.frame(do.call(rbind, AICs), row.names=sapply(model_ensemble, function(x) x$label)), col.names=sapply(model_ensemble, function(x) x$label),
      'latex', booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```

We can see how the lowest AIC value (which in this case is negative) for each one of the models corresponds to the itself so we can conclude that we are selecting the models correctly. By plotting the fitted functions against the test data we obtain the following:


```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_test_functions <- lapply (1:length(test_functions), function(i) data.frame(
  n=1:num_test_observations,
  d=sapply (1:num_test_observations, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(test_seqs, lapply(1:length(best_models), function(i) paste("Model ", model_ensemble[[i]]$label, " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_test_functions), line_colors=c("red"))
```
We can see how they fit perfectly. To avoid additional tables we have avoided including a comparison table for the parameters, but as it can be seen in the charts the nle was able to fit them almoet perfectly. Note that it can happen (there's some randomness in the results due to the noise and the optimizer) that the model selected as the best fitting for Model 2 is Model 3. This is because Model 3 is a more general case of Model 2 (to obtain Model 2 simply make $a = (1/2)^b$ in Model 3), so it can happen than Model 3 is able to find a value of $a$ around $a = (1/2)^b$ that obtains a lower AIC by fitting some of the noise.
# <NOTE> Comparison between non-average and average fitted lines. For non-averaged it is always model 2, for averaged is not always the case
