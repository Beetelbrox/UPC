---
output:
  pdf_document: default
  html_document: default
---
 ---
title: "Lab 04 - Non-linear regression on dependency trees"
author: "Francisco Javier Jurado, Roger Pujol Torramorell"
date: "October 29, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
# Load and install necessary packages
requiredPackages <- c("knitr", "rstudioapi", "kableExtra", "stats")

for (pac in requiredPackages) {
    if(!require(pac,  character.only=TRUE)){
        install.packages(pac, repos="http://cran.rstudio.com")
        library(pac,  character.only=TRUE)
    }
}
rm(pac)
rm(requiredPackages)

# set pwd to current directory, must load rstudioapi before. Need to check availability of API to avoid issues when knitting
if (rstudioapi::isAvailable()) setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

```

```{r, echo=FALSE}
# Model Class definition
model <- setRefClass("model", fields=list(label="character", params="ANY", f="function", formula="ANY", log_formula="ANY"))

# List of models
model_ensemble <- list(
  model(label="0",
        params=c(),
        f=function() { function(n) n/3 + 1/3 },
        formula=NA,
        log_formula=NA),
  
  model(label="1",
        params=c('b'),
        f=function(b) { function(n) (n/2)^b },
        formula=d~(n/2)^b,
        log_formula=log(d)~log(n/2) + 0),
  
  model(label="2",
        params=c('a', 'b'),
        f=function(a, b) { function(n) a*n^b },
        formula=d~a*n^b,
        log_formula=log(d)~log(n)),
  
  model(label="3",
        params=c('a','c'),
        f=function(a, c) { function(n) a*exp(c*n) },
        formula=d~a*exp(c*n),
        log_formula=log(d)~n),
  
  model(label="4",
        params=c('a'),
        f=function(a) { function(n) a*log(n) },
        formula=d~a*log(n),
        log_formula=log(d)~1*log(log(n)))
  )
```

```{r, echo=FALSE}
plot_grid <- function(data, labels, xlab, ylab, color="blue", lines=list(), line_colors=c(), log=FALSE) {
  old.par <- par(mfrow=c(ceiling(length(data)/3), 3))
  par(mar=c(4,4,2,2))
  for (i in 1:length(data)) {
    plot(x=data[[i]]$n,
         y=data[[i]]$d,
         main=labels[[i]],
         log= if(log) "xy" else '',
         xlab=xlab,
         ylab=ylab,
         col=color)
    for (j in seq_along(lines)) {
        lines(x=lines[[j]][[i]]$n,
        y=lines[[j]][[i]]$d,
        col=line_colors[[j]])
    }
  }
}
```


```{r, echo=FALSE,include=FALSE}
# Possible solutions for heteroscedasticity: work on smoothed versions of the data: eg. average all points for a value x and work on that

# plot everything to be plotted
# Check soundness by generating perfect synthetic data and checking the fit against it. R doesnt like perfect function, some error must be added to the actual data

# replace d with a z score transformation of d
# This transform could help to reduce noise in te data

```

```{r, echo=FALSE}
  # Read the list of languages from file
filename_suffix <- "_dependency_tree_metrics.txt"
language_list <- as.vector(read.table("data/language_list.txt", header = FALSE)$V1)

# Push the in-degree sequences and theh list of entries to the global scope
dep_tree_cols <- c('n', 'k^2', 'd')
metric_seqs <- structure(lapply(language_list, function(x) data.frame(read.table(paste("data/", x, filename_suffix, sep=''), header = FALSE, col.names=dep_tree_cols))),
                         names=language_list)

avg_seqs <- lapply(metric_seqs, function(x) aggregate(x, list(x$n), mean))
```

# Results
## On the validity of the inputs
```{r, echo=FALSE}
valid_k.2 <- lapply(dep_tree_metric_seqs, function(x) (4-(6/x$n) <= x$k.2) & (x$k.2 <= x$n-1))
valid_d <- lapply(dep_tree_metric_seqs, function(x) (x$k.2*x$n/(8*(x$n-1)) + 1/2 <= x$d) & (x$d <= x$n-1))
```
## Results
The first table in this results section summarizes the properties of the degree sequences, in particular the sample size and the mean and standard deviation of both the number of vertices $n$ and the mean dependency length $d$:
```{r, echo=FALSE}
summary_cols <- c('N', 'mean_n', 'sd_n', 'mean_d', 'sd_d')
metric_stats <- lapply(dep_tree_metric_seqs, function(x) list(
                    N=length(x),
                    mean_n=mean(x$n),
                    sd_n=sd(x$n),
                    mean_d=mean(x$d),
                    sd_d=sd(x$d)))

summary_df <- data.frame(do.call(rbind, lapply(metric_stats, function(x) c(x$N, x$mean_n, x$sd_n, x$mean_d, x$sd_d))), row.names=language_list)
kable(summary_df, 'latex', col.names=summary_cols, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```
TODO: COMMENT THE TABLE

\newpage
Before going any further it is a good practice to check what the data looks like so let's plot the mean depencency length $d$ vs the number of vertices $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, '# vertices', 'mean dependency length')
```
\newpage
We would like to check for any possible power-law dependencies and we can do so by plotting data taking logs on both axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(mean dependency length)', log=TRUE)
```
Note that rather than applying the log to the data itself we have set the plot axes to logarithmic, distributing the ticks in a log fashion. We can now observe that the plots suggest a power-law despite the large amount of dispersion.

\newpage
A way to deal with this dispersion and get a clearer intuition on the underlying trend is to average the mean length for a given number of vertices:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, '# vertices', 'avg mean dependency length')
```
Although there is stil a significant amount of dispersion for larger values of $n$, we have a way clearer view of the distribution shape. By plotting the same averaged points on log-log axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', log=TRUE)
```
The data points form an almost straight line in the log-log plot (again with dispersion when $n$ gets large) so we have reasonable evidence to believe they follow a power-law distribution.  

We now want to compare how far the real scaling of $d$ is from the one existing at a random linear arrangement. For that purpose we can compare the points to the averaged ones and the expected mean length, given by $E[\langle d \rangle] = (n+1)/3$. Plotting that in a regular and double logarithmic scale:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue",
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue", log=TRUE,
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

```{r}
generate_nlms <- function(seqs, ensemble) {
  lapply( seqs, function(seq) {
    # Inner loop 1 to iterate over the formulas
    lapply(ensemble, function(model) {
      if (length(model$params) > 0) {
        lm <- lm(model$log_formula, seq)
        initial_values <- structure(lapply(1:length(model$params), function(i)
          if (model$params[i] == "a") exp(coef(lm)[[i]]) else coef(lm)[[i]] ), names=model$params)
        nls( formula=model$formula, data=seq, start=initial_values, trace=FALSE)
      } else NA
    })
  })
}

calculate_deviances <- function(seqs, models) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { deviance(models[[i]][[j]]) }
      else { sum( (seqs[[i]]$d - (seqs[[i]]$n+1)/3)^2) }
    })
  })
}

calculate_AICs <- function(seqs, models, devs) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { AIC(models[[i]][[j]]) }
      else {
        n <- length(seqs[[i]]$n)
        n*log(2*pi) + n*log(devs[[i]][[j]]/n) + n + 2
      }
    })
  })
}
```

```{r}

nlms <- generate_nlms(seqs = avg_seqs, model_ensemble)
deviances <- calculate_deviances(avg_seqs, nlms)
AICs <- calculate_AICs(avg_seqs, nlms, deviances)

kable(data.frame(do.call(rbind, AICs), row.names = language_list),  col.names=sapply(model_ensemble, function(x) x$label), booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```


```{r, echo=FALSE}
best_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else NA))
best_model <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(language_list), function(i) best_params[[i]][[ as.numeric(best_model[i]) ]])
```

```{r, echo=FALSE}
#best_parameters <- lapply(as.numeric(which.min(sapply(nl_models, delta_AIC_df[i,]))
best_params <- lapply(1:length(language_list), function(i) 
  sapply(1:length(model_labels), function (j)
    if (length(params_ensemble[[j]]) > 0) coef(nl_models[[i]][[j]]) else NA
  )
)
```

```{r, echo=FALSE}
# Generate deviances
model_deviances <- lapply( seq(from=1, to=length(dep_tree_metric_seqs), by=1), function(i) {
  sapply( seq(from=1, to=length(nl_models[[i]]), by=1), function(j) {
    if ( length(params_ensemble[[j]]) > 0 ) {
      deviance(nl_models[[i]][[j]]) }
    else {
      sum( (dep_tree_metric_seqs[[i]]$d - (dep_tree_metric_seqs[[i]]$n+1)/3)^2)
    }
  })
})

# Generate AICs
model_AICs <- lapply( seq(from=1, to=length(dep_tree_metric_seqs), by=1), function(i) {
  sapply( seq(from=1, to=length(nl_models[[i]]), by=1), function(j) {
    if ( length(params_ensemble[[j]]) > 0 ) {
      AIC(nl_models[[i]][[j]]) }
    else {
      n <- length(dep_tree_metric_seqs[[i]]$n)
      n*log(2*pi) + n*log(model_deviances[[i]][[j]]/n) + n + 2
    }
  })
})

kable(data.frame(do.call(rbind, model_AICs), row.names = language_list),  col.names=model_labels, booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```
```{r}
list(best_coefs[[1]])[[1]]["c"]
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(metric_seqs)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(metric_seqs), by=1)) {
  plot(x=metric_seqs[[i]]$n,
       y=metric_seqs[[i]]$d,
       main=language_list[[i]],
       #log='xy',
       xlab='log(# vertices)',
       ylab='log(mean dependency length)',
       col='blue')
  lines(x=avg_seqs[[i]]$n,
        y=do.call (model_ensemble[[ as.numeric(best_model[i]) ]]$f, as.list(best_coefs[[i]]))(avg_seqs[[i]]$n),
        col="red"
  )
  
    # Add the average sequence values
  lines(x=avg_seqs[[i]]$n,
        y=avg_seqs[[i]]$d,
        col="green")
}

```

```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(metric_seqs)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(metric_seqs), by=1)) {
  plot(x=metric_seqs[[i]]$n,
       y=metric_seqs[[i]]$d,
       main=language_list[[i]],
       log='xy',
       xlab='log(# vertices)',
       ylab='log(mean dependency length)',
       col='blue')
  lines(x=avg_seqs[[i]]$n,
        y=do.call (model_ensemble[[ as.numeric(best_model[i]) ]]$f, as.list(best_coefs[[i]]))(avg_seqs[[i]]$n),
        col="red"
  )
  
    # Add the average sequence values
  lines(x=avg_seqs[[i]]$n,
        y=avg_seqs[[i]]$d,
        col="green")
}

```