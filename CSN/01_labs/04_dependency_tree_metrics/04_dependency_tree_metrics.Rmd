---
output:
  pdf_document: default
  html_document: default
---
---
title: "Lab 04 - Non-linear regression on dependency trees"
author: "Francisco Javier Jurado, Roger Pujol Torramorell"
date: "October 29, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
# Load and install necessary packages
requiredPackages <- c("knitr", "rstudioapi", "kableExtra", "stats")

for (pac in requiredPackages) {
    if(!require(pac,  character.only=TRUE)){
        install.packages(pac, repos="http://cran.rstudio.com")
        library(pac,  character.only=TRUE)
    }
}
rm(pac)
rm(requiredPackages)

# set pwd to current directory, must load rstudioapi before. Need to check availability of API to avoid issues when knitting
if (rstudioapi::isAvailable()) setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

```

```{r, echo=FALSE}
# Model Class definition
model <- setRefClass("model", fields=list(label="character", params="ANY", f="function", formula="ANY", log_formula="ANY", lower="ANY"))

# List of models. Each model is an object. f returns the model function given its parameters to be able to generate points
model_ensemble <- list(
  model(label="0",
        params=c(),
        f=function() { function(n) n/3 + 1/3 },
        formula=NA,
        log_formula=NA),

  model(label="1",
        params=c('b'),
        f=function(b) { function(n) (n/2)^b },
        formula=d~(n/2)^b,
        log_formula=function(d0) { log(d)~log(n/2) + 0 }),

  model(label="2",
        params=c('a', 'b'),
        f=function(a, b) { function(n) a*n^b },
        formula=d~a*n^b,
        log_formula=function(d0) { log(d)~log(n) }),

  model(label="3",
        params=c('a','c'),
        f=function(a, c) { function(n) a*exp(c*n) },
        formula=d~a*exp(c*n),
        log_formula=function(d0) { log(d)~n }),

  model(label="4",
        params=c('a'),
        f=function(a) { function(n) a*log(n) },
        formula=d~a*log(n),
        log_formula=function(d0) { log(d)~1*log(log(n)) } ),

  model(label="1+",
        params=c('b', 'e'),
        f=function(b, e) { function(n) (n/2)^b + e},
        formula=d~(n/2)^b + e,
        log_formula=function(d0) { log(d-d0)~log(n/2) + 0 }),
  
    model(label="2+",
        params=c('a', 'b', 'e'),
        f=function(a, b, e) { function(n) a*n^b + e},
        formula=d~a*n^b + e,
        log_formula=function(d0) { log(d-d0)~log(n) }),
  
  model(label="3+",
        params=c('a','c', 'e'),
        f=function(a, c, e) { function(n) a*exp(c*n) + e },
        formula=d~a*exp(c*n) + e,
        log_formula=function(d0) { log(d-d0)~n })
)
```

```{r, echo=FALSE}
generate_nlms <- function(seqs, ensemble) {
  lapply( seqs, function(seq) {         # Outer loop to iterate over the dependency sequences
    d0 <- min(seq$d/2)
    lapply(ensemble, function(model) {  # Inner loop to iterate over the models
      if (length(model$params) > 0) {
        lm <- lm(model$log_formula(d0), seq)
        initial_values <- structure(lapply(1:length(model$params), function(i)
          if (model$params[i] == "a") exp(coef(lm)[[i]]) else if (model$params[i] == "e") d0 else coef(lm)[[i]] ), names=model$params)
        nls( formula=model$formula, data=seq, start=initial_values, control=nls.control(maxiter=1000, warnOnly = TRUE), trace=FALSE)
      } else NA
    })
  })
}

calculate_deviances <- function(seqs, models) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { deviance(models[[i]][[j]]) }
      else { sum( (seqs[[i]]$d - (seqs[[i]]$n+1)/3)^2) }
    })
  })
}

calculate_AICs <- function(seqs, models, devs) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { AIC(models[[i]][[j]]) }
      else {
        n <- length(seqs[[i]]$n)
        n*log(2*pi) + n*log(devs[[i]][[j]]/n) + n + 2
      }
    })
  })
}

# This pushes tables to the global context
generate_models_and_measures <- function (seqs, ensemble) {
  nlms <<- generate_nlms(seqs=seqs, ensemble=ensemble)
  deviances <<- calculate_deviances(seqs, nlms)
  AICs <<- calculate_AICs(seqs, nlms, deviances)
}

plot_grid <- function(data, labels, xlab, ylab, color="blue", lines=list(), line_colors=c(), log=FALSE) {
  old.par <- par(mfrow=c(ceiling(length(data)/3), 3))
  par(mar=c(4,4,2,2))
  for (i in 1:length(data)) {
    plot(x=data[[i]]$n,
         y=data[[i]]$d,
         main=labels[[i]],
         log= if(log) "xy" else '',
         xlab=xlab,
         ylab=ylab,
         col=color)
    for (j in seq_along(lines)) {
        lines(x=lines[[j]][[i]]$n,
        y=lines[[j]][[i]]$d,
        col=line_colors[[j]])
    }
  }
}
```

```{r, echo=FALSE}
###########################################################################################
# Reading the data from file
###########################################################################################
  # Read the list of languages from file
filename_suffix <- "_dependency_tree_metrics.txt"
language_list <- as.vector(read.table("data/language_list.txt", header = FALSE)$V1)

# Push the in-degree sequences and theh list of entries to the global scope
dep_tree_cols <- c('n', 'k^2', 'd')
metric_seqs_raw <- lapply(structure(lapply(language_list, function(x) data.frame(read.table(paste("data/", x, filename_suffix, sep=''), header = FALSE, col.names=dep_tree_cols))),
                         names=language_list), function(lang) lang[order(lang$n),])

###########################################################################################
# Generation of the testing sequences
###########################################################################################

# Hard-coded list of parameters and number of observations
test_params <- list(c(), c(b=0.5), c(a=0.8, b=0.5), c(a=0.8, c=0.01), c(a=0.8), c(b=0.5, e=2), c(a=0.8, b=0.5, e=2), c(a=0.8, c=0.01, e=2))
max_n_test <- 200
test_n <- 2:max_n_test
# Generate the functions with the provided parameters
test_functions <- lapply(1:length(test_params), function(i) do.call(model_ensemble[[i]]$f, as.list(test_params[[i]])))
# Generate the test sequences of length num_test_observations. To avoid issues with the optimizer we add some gaussian error
test_seqs <- lapply(test_functions, function(f) data.frame(
  n=test_n,
  d=sapply(sapply(test_n, function(i) f(i)) + rnorm(length(test_n), sd=0.02), function(x) x))) # The nls freaks out w/ values close to 0 so we force all values to be at least 1 (after all, the min distance is 1  )
```

# Results

## On input validity
```{r, echo=FALSE}
valid_k.2 <- lapply(metric_seqs_raw, function(x) (4-(6/x$n) <= x$k.2) & (x$k.2 <= x$n-1))
valid_d <- lapply(metric_seqs_raw, function(x) (x$k.2*x$n/(8*(x$n-1)) + 1/2 <= x$d) & (x$d <= x$n/2))
```

As the first step in our data analysis process we need to check that the values of $\langle d \rangle$ and $\langle k^2 \rangle$ provided in the input data satisfy the following conditions:

\[
4-6/n \leq \langle k^2 \rangle \leq n-1
\]

and

\[
\frac{n}{8(n-1)}\langle k^2 \rangle + \frac{1}{2} \leq \langle d \rangle \leq n-1
\]

As we are working with $\langle d \rangle$ and its bounds depend on the value of $\langle k^2 \rangle$ we can deem a row as invalid if it does not satisfy any of the conditions. By checking validity of both measures for all languages, we obtain the following table:
The rows that do not qualify

```{r, echo=FALSE}
invalid_cols <- c('# invalid rows k^2', '# invalid rows d')
invalid_rows <- lapply(language_list, function(lang) c( sum(!valid_k.2[[lang]]), sum(!valid_d[[lang]]) ) )
invalid_df <- data.frame(do.call(rbind, invalid_rows), row.names=language_list)
kable(invalid_df, 'latex', col.names=invalid_cols, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```

We can observe how the all the invalid rows are due to the $\langle k^2 \rangle$ parameter. This is not coincidental, as if we take a closer look to which side of the inequality is not satisfied we find out that for all cases the value of $\langle k^2 \rangle$ is smaller than the lower bound, with the consequence of lowering $\langle d \rangle$'s lower bound too. With regards to upper bound, they are not violated in any case. Even if we a apply a tighter bound for $\langle d \rangle$, $\langle k^2 \rangle \leq n/2$ (upper bound of $\langle d \rangle$ for non-crossing trees) no row breaks this condition. A final remark is that all but 3 invalid entries (over all languages) happen for $n=9$, which is definitely an unusual finding. Although there are some relation between the number of observations and incidences (the first and second largest amount of invalid rows happen for the first and second languages with the largst number of trees in the dataset) they are not consistent over all languages. As the amount of invalid rows is very small compared to the overall amount of entries, we have opted for removing them from the dataset before proceeding with the fitting. 

```{r, echo=FALSE}
metric_seqs <- lapply(language_list, function(lang) metric_seqs_raw[[lang]][valid_k.2[[lang]] & valid_d[[lang]],] )
avg_seqs <- lapply(metric_seqs, function(x) aggregate(x, list(x$n), mean))
```

## Properties summary
Now that the data is clean it would be interesting to have an overall view on the properties of the degree sequences, in particular the sample size and the mean and standard deviation of both the number of vertices $n$ and the mean dependency length $d$:
```{r, echo=FALSE}
summary_cols <- c('N', 'mean_n', 'sd_n', 'mean_d', 'sd_d')
metric_stats <- lapply(metric_seqs, function(x) list(
                    N=length(x$n),
                    mean_n=mean(x$n),
                    sd_n=sd(x$n),
                    mean_d=mean(x$d),
                    sd_d=sd(x$d)))

summary_df <- data.frame(do.call(rbind, lapply(metric_stats, function(x) c(x$N, x$mean_n, x$sd_n, x$mean_d, x$sd_d))), row.names=language_list)
kable(summary_df, col.names=summary_cols, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```
The first thing we can observe is that the amount of syntactic trees available for each language is not equal, and the relative amount does not immediately correpsond to the relative amount of that language's speakers. To be able to better compare the means and standard deviations let's plot the corresponding boxplots for the distributions od $n$ and $\langle d \rangle$

```{r, echo=FALSE, fig.width=16, fig.height=8}
boxplot(structure(lapply(metric_seqs, function(seq) seq$n), names=language_list), ylab='# nodes', main='Number of nodes (n)')
```

Most languages' average number of nodes range from 16 to 26, with the exception of Chinese, Basque and Turkish which have a very low value compared to the others. Standard deviation varies significantly between languages, Arabic having a sd almost equal to its mean and English's being less than half its mean.  

```{r, echo=FALSE, fig.width=16, fig.height=8}
boxplot(structure(lapply(metric_seqs, function(seq) seq$d), names=language_list), ylab='mean dependency length', main='Mean Dependency length (d)')
```

With regard to the mean dependency length most languages have a value around 2, with the exception of English (3.05), Hungarian (3.87) and Chinese (1.44). Standard deviation is somewhat consistent around 0.8, Hungarian and chinese being the exception with values 1.78 and 0.48 respectively.  
For both values the mayority of outliers lie above the quartile line, which makes sense as negative lengths are not possible.

\newpage
## Preliminary visualization
Before going any further it is a good practice to check what the data looks like, so let's plot the mean depencency length $\langle d \rangle$ vs the number of vertices $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, '# vertices', 'mean dependency length')
```

\newpage
We would like to check for any possible power-law dependencies and we can do so by plotting data taking logs on both axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(mean dependency length)', log=TRUE)
```
Note that rather than applying the log to the data itself we have set the plot axes to logarithmic, distributing the ticks in a log fashion. The plots suggest a power-law distribution of We can now observe that the plots suggest that $\langle d \rangle$ follows a power-law distribution, although there is a significant amount of dispersion and it increases for large values of $n$. Notice that the scale is quite different for different languages, so what seem like large variations for some languages are not such when compared against the overall range of $\langle d \rangle$.

\newpage
## Checking for heteroscedasticity
In the previous charts we observed a significant amount of dispersion for $\langle d \rangle$'s values, and for some languages it increased for large values of $n$. As the non-linear models that we will be using assume constant variance (Homoscedasticity), it's worth to check if that is the case by plotting the value of $\langle d \rangle$'s variance for different values of $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
# Careful, this contains
var_seqs <- lapply(1:length(metric_seqs), function(i) data.frame(n=avg_seqs[[i]]$n, d=sapply(avg_seqs[[i]]$n, function(x) var( metric_seqs[[i]][metric_seqs[[i]]$n == x,]$d) ) ))
plot_grid(var_seqs, language_list, '# vertices', 'mean dependency length variance')
```

Note that in order to calculate the variance for a given value of $n$ we need more than one data point so those $n$ with a single point have been omitted. Before commenting on the plots take into consideration that the range of values varies wildly from one language to another, so what it might seem like a large variance it's actually not when compared to the largest range over all languages (eg chinese seems to have a large amount of heteroscedas)

\newpage
A way to deal with this dispersion and get a clearer intuition on the underlying trend is to average the mean length for a given number of vertices:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, '# vertices', 'avg mean dependency length')
```
Although there is stil a significant amount of dispersion for larger values of $n$, we have a way clearer view of the distribution shape. By plotting the same averaged points on log-log axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', log=TRUE)
```
The data points form an almost straight line in the log-log plot (again with dispersion when $n$ gets large) so we have reasonable evidence to believe they follow a power-law distribution.  

We now want to compare how far the real scaling of $d$ is from the one existing at a random linear arrangement. For that purpose we can compare the points to the averaged ones and the expected mean length, given by $E[\langle d \rangle] = (n+1)/3$. Plotting that in a regular and double logarithmic scale:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue",
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue", log=TRUE,
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

## On correctness
```{r}
generate_models_and_measures(metric_seqs, model_ensemble)
fitted_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else c()))
best_models <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(nlms), function(i) fitted_params[[i]][[ as.numeric(best_models[i]) ]])
```
```{r, echo=FALSE}
kable(data.frame(do.call(rbind, AICs), row.names=language_list), col.names=sapply(model_ensemble, function(x) x$label),
       booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_metric_functions <- lapply (1:length(avg_seqs), function(i) data.frame(
  n=avg_seqs[[i]]$n,
  d=sapply (avg_seqs[[i]]$n, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(metric_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_metric_functions), line_colors=c("red"))
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_metric_functions <- lapply (1:length(avg_seqs), function(i) data.frame(
  n=avg_seqs[[i]]$n,
  d=sapply (avg_seqs[[i]]$n, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(metric_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue", 
          lines=list(fitted_metric_functions), line_colors=c("red"), log=TRUE)
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_metric_functions), line_colors=c("red"))
```

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue", 
          lines=list(fitted_metric_functions), line_colors=c("red"), log=TRUE)
```

```{r, echo=FALSE}
generate_models_and_measures(test_seqs, model_ensemble)
fitted_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else c()))
best_models <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(nlms), function(i) fitted_params[[i]][[ as.numeric(best_models[i]) ]])
```

Before trying to select the best model for each real dependency length sequence we would like to make sure our model selection pipeline works properly. In order to test it we generated test sequences for each one of the models in our ensemble following the actual function of each model. With regards to the parameters of the models we chose the following values (no particular criteria, we made an educated guess based on the values obtained when fitting on the real data): $a=0.8$, $b=0.5$, $c=0.01$.  
Because the optimizer has some difficulties trying to fit data that perfectly matches the function being fitted (reaches max number of iterations before convergence due to precision issues) we have added some gaussian noise as suggested in the lab session. We wanted keep the data as close as possible to the real functions so after trying several parameters we chose a $mean=0$ and $sd=0.05$ as the noise's gausian distribution parameters. By plotting the synthetic data (with linear axes) we can see the shapes of the functions in our ensemble:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(test_seqs, lapply(model_ensemble, function(x) paste("Model ", x$label)), xlab='n', ylab='f(n)')
```

Now that we have our testing data we can fit the models and calculate the AIC for each one of them:
```{r, echo=FALSE}
kable(data.frame(do.call(rbind, AICs), row.names=sapply(model_ensemble, function(x) x$label)), col.names=sapply(model_ensemble, function(x) x$label),
       booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```




We can see how the lowest AIC value (which in this case is negative) for each one of the models corresponds to the itself so we can conclude that we are selecting the models correctly. By plotting the fitted functions against the test data we obtain the following:

```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_test_functions <- lapply (1:length(test_functions), function(i) data.frame(
  n=test_n,
  d=sapply (test_n, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(test_seqs, lapply(1:length(best_models), function(i) paste("Model ", model_ensemble[[i]]$label, " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_test_functions), line_colors=c("red"))
```
We can see how they fit perfectly. To avoid additional tables we have avoided including a comparison table for the parameters, but as it can be seen in the charts the nle was able to fit them almoet perfectly. Note that it can happen (there's some randomness in the results due to the noise and the optimizer) that the model selected as the best fitting for Model 2 is Model 3. This is because Model 3 is a more general case of Model 2 (to obtain Model 2 simply make $a = (1/2)^b$ in Model 3), so it can happen than Model 3 is able to find a value of $a$ around $a = (1/2)^b$ that obtains a lower AIC by fitting some of the noise.
# <NOTE> Comparison between non-average and average fitted lines. For non-averaged it is always model 2, for averaged is not always the case



