---
output:
  pdf_document: default
  html_document: default
---
---
title: "Lab 04 - Non-linear regression on dependency trees"
author: "Francisco Javier Jurado, Roger Pujol Torramorell"
date: "October 29, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, include=FALSE}
# Load and install necessary packages
requiredPackages <- c("knitr", "rstudioapi", "kableExtra", "stats")

for (pac in requiredPackages) {
    if(!require(pac,  character.only=TRUE)){
        install.packages(pac, repos="http://cran.rstudio.com")
        library(pac,  character.only=TRUE)
    }
}
rm(pac)
rm(requiredPackages)

# set pwd to current directory, must load rstudioapi before. Need to check availability of API to avoid issues when knitting
if (rstudioapi::isAvailable()) setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

```

```{r, echo=FALSE, include=FALSE}
# Model Class definition
model <- setRefClass("model", fields=list(label="character", params="ANY", f="function", formula="ANY", log_formula="ANY", lower="ANY"))

# List of models. Each model is an object. f returns the model function given its parameters to be able to generate points
model_ensemble <- list(
  model(label="0",
        params=c(),
        f=function() { function(n) n/3 + 1/3 },
        formula=NA,
        log_formula=NA),

  model(label="1",
        params=c('b'),
        f=function(b) { function(n) (n/2)^b },
        formula=d~(n/2)^b,
        log_formula=function(d0) { log(d)~log(n/2) + 0 }),

  model(label="2",
        params=c('a', 'b'),
        f=function(a, b) { function(n) a*n^b },
        formula=d~a*n^b,
        log_formula=function(d0) { log(d)~log(n) }),

  model(label="3",
        params=c('a','c'),
        f=function(a, c) { function(n) a*exp(c*n) },
        formula=d~a*exp(c*n),
        log_formula=function(d0) { log(d)~n }),

  model(label="4",
        params=c('a'),
        f=function(a) { function(n) a*log(n) },
        formula=d~a*log(n),
        log_formula=function(d0) { log(d)~1*log(log(n)) } ),
  
    model(label="5",
        params=c('a', 'b', 'c'),
        f=function(a, b, c) { function(n) a*(n^b)*exp(c*n) },
        formula=d~a*(n^b)*exp(c*n),
        log_formula=function(d0) { log(d)~log(n) + n }),

  model(label="1+",
        params=c('b', 'e'),
        f=function(b, e) { function(n) (n/2)^b + e},
        formula=d~(n/2)^b + e,
        log_formula=function(d0) { log(d-d0)~log(n/2) + 0 }),
  
    model(label="2+",
        params=c('a', 'b', 'e'),
        f=function(a, b, e) { function(n) a*n^b + e},
        formula=d~a*n^b + e,
        log_formula=function(d0) { log(d-d0)~log(n) }),
  
  model(label="3+",
        params=c('a','c', 'e'),
        f=function(a, c, e) { function(n) a*exp(c*n) + e },
        formula=d~a*exp(c*n) + e,
        log_formula=function(d0) { log(d-d0)~n }),
  
    model(label="4+",
        params=c('a', 'e'),
        f=function(a, e) { function(n) a*log(n) + e},
        formula=d~a*log(n) + e,
        log_formula=function(d0) { log(d-d0)~1*log(log(n)) } ),
  
      model(label="5+",
        params=c('a', 'b', 'c', 'e'),
        f=function(a, b, c, e) { function(n) a*(n^b)*exp(c*n) + e },
        formula=d~a*(n^b)*exp(c*n) + e,
        log_formula=function(d0) { log(d-d0)~log(n) + n })
)
```

```{r, echo=FALSE, include=FALSE}
generate_nlms <- function(seqs, ensemble) {
  lapply( seqs, function(seq) {         # Outer loop to iterate over the dependency sequences
    d0 <- min(seq$d/2)
    lapply(ensemble, function(model) {  # Inner loop to iterate over the models
      if (length(model$params) > 0) {
        lm <- lm(model$log_formula(d0), seq)
        initial_values <- structure(lapply(1:length(model$params), function(i)
          if (model$params[i] == "a") exp(coef(lm)[[i]]) else if (model$params[i] == "e") d0 else coef(lm)[[i]] ), names=model$params)
        nls( formula=model$formula, data=seq, start=initial_values, control=nls.control(maxiter=1000, warnOnly = TRUE), trace=FALSE)
      } else NA
    })
  })
}

calculate_deviances <- function(seqs, models) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { deviance(models[[i]][[j]]) }
      else { sum( (seqs[[i]]$d - (seqs[[i]]$n+1)/3)^2) }
    })
  })
}

calculate_AICs <- function(seqs, models, devs) {
  lapply (1:length(seqs), function(i) {
    sapply(1:length(models[[i]]), function(j) {
      if (!is.na(models[[i]][j])) { AIC(models[[i]][[j]]) }
      else {
        n <- length(seqs[[i]]$n)
        n*log(2*pi) + n*log(devs[[i]][[j]]/n) + n + 2
      }
    })
  })
}

# This pushes tables to the global context
generate_models_and_measures <- function (seqs, ensemble) {
  nlms <<- generate_nlms(seqs=seqs, ensemble=ensemble)
  deviances <<- calculate_deviances(seqs, nlms)
  AICs <<- calculate_AICs(seqs, nlms, deviances)
}

plot_grid <- function(data, labels, xlab, ylab, color="blue", lines=list(), line_colors=c(), log=FALSE) {
  old.par <- par(mfrow=c(ceiling(length(data)/3), 3))
  par(mar=c(4,4,2,2))
  for (i in 1:length(data)) {
    plot(x=data[[i]]$n,
         y=data[[i]]$d,
         main=labels[[i]],
         log= if(log) "xy" else '',
         xlab=xlab,
         ylab=ylab,
         col=color)
    for (j in seq_along(lines)) {
        lines(x=lines[[j]][[i]]$n,
        y=lines[[j]][[i]]$d,
        col=line_colors[[j]])
    }
  }
}
```

```{r, echo=FALSE}
###########################################################################################
# Reading the data from file
###########################################################################################
  # Read the list of languages from file
filename_suffix <- "_dependency_tree_metrics.txt"
language_list <- as.vector(read.table("data/language_list.txt", header = FALSE)$V1)

# Push the in-degree sequences and theh list of entries to the global scope
dep_tree_cols <- c('n', 'k^2', 'd')
metric_seqs_raw <- lapply(structure(lapply(language_list, function(x) data.frame(read.table(paste("data/", x, filename_suffix, sep=''), header = FALSE, col.names=dep_tree_cols))),
                         names=language_list), function(lang) lang[order(lang$n),])

###########################################################################################
# Generation of the testing sequences
###########################################################################################

# Hard-coded list of parameters and number of observations
test_params <- list(
  c(),
  c(b=0.5),
  c(a=0.8, b=0.5),
  c(a=0.8, c=0.01),
  c(a=0.8),
  c(a=0.8, b=0.5, c=0.01),
  c(b=0.5, e=2),
  c(a=0.8, b=0.5, e=2),
  c(a=0.8, c=0.01, e=2),
  c(a=0.8, e=2),
  c(a=0.8, b=0.5, c=0.01, e=2)
)
max_n_test <- 200
test_n <- 2:max_n_test
# Generate the functions with the provided parameters
test_functions <- lapply(1:length(test_params), function(i) do.call(model_ensemble[[i]]$f, as.list(test_params[[i]])))
# Generate the test sequences of length num_test_observations. To avoid issues with the optimizer we add some gaussian error
test_seqs <- lapply(test_functions, function(f) data.frame(
  n=test_n,
  d=sapply(sapply(test_n, function(i) f(i)) + rnorm(length(test_n), sd=0.02), function(x) x))) # The nls freaks out w/ values close to 0 so we force all values to be at least 1 (after all, the min distance is 1  )
```

#Introduction

In this lab session, we are going to practice on the fit of a non-linear function to data using collections of syntactic dependency trees from different languages.
In asyntactic dependency trees, the vertices are the words (tokens) of a sentence, and links indicate syntactic dependencies between words. 

$n$ is defined as the number of vertices of a tree. $\langle k^2\rangle$ is defined as its degree 2nd moment and $\langle d\rangle$ as the mean length of its edges.
In our particular case, we are going to investigate the scaling of $\langle d\rangle$ as a function of $n$.

# Results

## On input validity
```{r, echo=FALSE}
valid_k.2 <- lapply(metric_seqs_raw, function(x) (4-(6/x$n) <= x$k.2) & (x$k.2 <= x$n-1))
valid_d <- lapply(metric_seqs_raw, function(x) (x$k.2*x$n/(8*(x$n-1)) + 1/2 <= x$d) & (x$d <= x$n/2))
```

As the first step in our data analysis process we need to check that the values of $\langle d \rangle$ and $\langle k^2 \rangle$ provided in the input data satisfy the following conditions:

\[
4-6/n \leq \langle k^2 \rangle \leq n-1
\]

and

\[
\frac{n}{8(n-1)}\langle k^2 \rangle + \frac{1}{2} \leq \langle d \rangle \leq n-1
\]

As we are working with $\langle d \rangle$ and its bounds depend on the value of $\langle k^2 \rangle$ we can deem a row as invalid if it does not satisfy any of the conditions. By checking validity of both measures for all languages, we obtain the following table:

```{r, echo=FALSE}
invalid_cols <- c('# invalid rows k^2', '# invalid rows d')
invalid_rows <- lapply(language_list, function(lang) c( sum(!valid_k.2[[lang]]), sum(!valid_d[[lang]]) ) )
invalid_df <- data.frame(do.call(rbind, invalid_rows), row.names=language_list)
kable(invalid_df,  col.names=invalid_cols, format='latex', booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```

We can observe how the all the invalid rows are due to the $\langle k^2 \rangle$ parameter. This is not coincidental, as if we take a closer look to which side of the inequality is not satisfied we find out that for all cases the value of $\langle k^2 \rangle$ is smaller than the lower bound, with the consequence of lowering $\langle d \rangle$'s lower bound too. With regards to upper bound, they are not violated in any case. Even if we a apply a tighter bound for $\langle d \rangle$, $\langle d \rangle \leq n/2$ (upper bound of $\langle d \rangle$ for non-crossing trees) no row breaks this condition. A final remark is that all but 3 invalid entries (over all languages) happen for $n=9$, which is definitely an unusual finding. Although there are some relation between the number of observations and incidences (the first and second largest amount of invalid rows happen for the first and second languages with the largst number of trees in the dataset) they are not consistent over all languages. As the amount of invalid rows is very small compared to the overall amount of entries, we have opted for removing them from the dataset before proceeding with the fitting. 

```{r, echo=FALSE}
metric_seqs <- lapply(language_list, function(lang) metric_seqs_raw[[lang]][valid_k.2[[lang]] & valid_d[[lang]],] )
avg_seqs <- lapply(metric_seqs, function(x) aggregate(x, list(x$n), mean))
```

## Properties summary
Now that the data is clean it would be interesting to have an overall view on the properties of the degree sequences, in particular the sample size and the mean and standard deviation of both the number of vertices $n$ and the mean dependency length $d$:
```{r, echo=FALSE}
summary_cols <- c('N', 'mean_n', 'sd_n', 'mean_d', 'sd_d')
metric_stats <- lapply(metric_seqs, function(x) list(
                    N=length(x$n),
                    mean_n=mean(x$n),
                    sd_n=sd(x$n),
                    mean_d=mean(x$d),
                    sd_d=sd(x$d)))

summary_df <- data.frame(do.call(rbind, lapply(metric_stats, function(x) c(x$N, x$mean_n, x$sd_n, x$mean_d, x$sd_d))), row.names=language_list)
kable(summary_df, col.names=summary_cols, format='latex', booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```
The first thing we can observe is that the amount of syntactic trees available for each language is not equal, and the relative amount does not immediately correpsond to the relative amount of speakers of said language.  
To be able to better compare the means and standard deviations let's plot the corresponding boxplots for the distributions of $n$ and $\langle d \rangle$

```{r, echo=FALSE, fig.width=16, fig.height=8}
boxplot(structure(lapply(metric_seqs, function(seq) seq$n), names=language_list), ylab='# nodes', main='Number of nodes (n)')
```

Most languages' average number of nodes range from 16 to 26, with the exception of Chinese, Basque and Turkish which have a very low value compared to the others. Standard deviation varies significantly between languages, Arabic having a sd almost equal to its mean and English's being less than half its mean.  

```{r, echo=FALSE, fig.width=16, fig.height=8}
boxplot(structure(lapply(metric_seqs, function(seq) seq$d), names=language_list), ylab='mean dependency length', main='Mean Dependency length (d)')
```

With regard to the mean dependency length most languages have a value around 2, with the exception of English (3.05), Hungarian (3.87) and Chinese (1.44). Standard deviation is somewhat consistent around 0.8, Hungarian and chinese being the exception with values 1.78 and 0.48 respectively.  
For both values the mayority of outliers lie above the quartile line, which makes sense as negative lengths are not possible.

\newpage
## Preliminary visualization
Before going any further it is a good practice to check what the data looks like, so let's plot the mean depencency length $\langle d \rangle$ vs the number of vertices $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, '# vertices', 'mean dependency length')
```

\newpage
We would like to check for any possible power-law dependencies and we can do so by plotting data taking logs on both axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(mean dependency length)', log=TRUE)
```

Note that rather than applying the log to the data itself we have set the plot axes to logarithmic, distributing the ticks in a log fashion. We can now observe that the plots suggest that $\langle d \rangle$ follows a power-law distribution, although there is a significant amount of dispersion and it increases for large values of $n$. Notice that the scale is quite different for different languages, so what seem like large variations for some languages might not be when compared with the overall range of $\langle d \rangle$.

\newpage
## Checking for heteroscedasticity
In the previous charts we observed a significant amount of dispersion for $\langle d \rangle$'s values, and for some languages it increased for large values of $n$. As the non-linear models that we will be using assume constant variance (Homoscedasticity), it's worth to check if that is the case by plotting the value of $\langle d \rangle$'s variance for different values of $n$:

```{r, echo=FALSE, fig.width=16, fig.height=18}
# Careful, this contains
var_seqs <- lapply(1:length(metric_seqs), function(i) data.frame(n=avg_seqs[[i]]$n, d=sapply(avg_seqs[[i]]$n, function(x) var( metric_seqs[[i]][metric_seqs[[i]]$n == x,]$d) ) ))
plot_grid(var_seqs, language_list, '# vertices', 'mean dependency length variance' )
```

Note that in order to calculate the variance for a given value of $n$ we need more than one data point so those $n$ with a single point have been omitted. Before performing the visual comparison, take into account that the scale of the y axis on the plots varies significantly from one language to another, so it needs to be taken into consideration when evaluating the amount of heteroscedasticity. We can see how several languages have relatively constant variance except from several isolated points, while others like Tuskish and Chinese present a way more visible variation as $n$ increases, and because of this we might want to consider some method to deal with this fact.  

A way of trying to visualize any underlying trend in the data and to somewhat alleviate the heteroscedasticity issue is to work on the average mean length for a given number $n$ of vertices instead of the whole dataset:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, '# vertices', 'avg mean dependency length')
```

Although there is stil a significant amount of dispersion for larger values of $n$, we have a way clearer view of the distribution shape, although with far less data points.

\newpage
By plotting the same averaged points on a log-log axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', log=TRUE)
```

The data points form an almost straight line in the log-log plot (again with dispersion when $n$ gets large) so we have reasonable evidence to believe they follow a power-law distribution and that heteroscedasticity could indeed be an issue to take into consideration.

\newpage
## Real vs random scaling of d

We now want to compare how far the real scaling of $\langle d \rangle$ is from the one existing at a random linear arrangement. For that purpose we can compare the points to the averaged ones and the expected mean length, given by $E[\langle d \rangle] = (n+1)/3$. Plotting the expected and empirical mean length together with the data points:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue",
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

Expected mean length is plotted in red, empirical averaged mean length is plotted in dark blue and empirical mean length values is plotted in light blue for readability purposes in all comparison plots from now on.

\newpage
Plotting the same lines on log-log charts:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(metric_seqs, language_list, 'log(# vertices)', 'log(avg mean dependency length)', color="light blue", log=TRUE,
          lines=list(avg_seqs, lapply(avg_seqs, function(x) list("n"=x$n, "d"=(x$n+1)/3))), line_colors=c("blue", "red"))
```

We can see how the average trend scales sublinearly with the number of nodes $n$, which agrees with what was explained in theory lectures about $\langle d \rangle$ being minimized in real trees, most likely to reduce cognitive load.

\newpage
## On correctness



```{r, echo=FALSE, include=FALSE}
generate_models_and_measures(test_seqs, model_ensemble);
fitted_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else c()))
best_models <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(nlms), function(i) fitted_params[[i]][[ as.numeric(best_models[i]) ]])
```

Before trying to select the best model for each real dependency length sequence we would like to make sure our model selection pipeline works properly. In order to test or methods we can make use of synthetic data generated using the functions $f(n, params)$ of each one of the models in our ensemble. As values for the models' parameters we chose the following values (no particular criteria, we made an educated guess based on the values obtained when fitting on the real data): $a=0.8$, $b=0.5$, $c=0.01$ and $e=2$.

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(test_seqs, lapply(model_ensemble, function(x) paste("Model ", x$label)), xlab='n', ylab='f(n)')
```

Because the nle optimizer has difficulties fitting functions to data that perfectly matches them we added additive gaussian noise (0 mean, 0.02 sd) to each data point as suggested.

\newpage
With the testing data ready, we can fit each one of our models to it and see how they perform. By calculating the AIC we obtain the following table:

```{r, echo=FALSE, fig.width=14}
kable(data.frame(do.call(rbind, AICs), row.names=sapply(model_ensemble, function(x) x$label)), col.names=sapply(model_ensemble, function(x) x$label),
       format='latex', booktabs=T, linesep='', align='c', digits=2 ) %>%
  kable_styling(latex_options=c("striped", "scale_down"))
```

We can see how the lowest AIC value (which in this case is negative) for each one of the models is indeed either itself or a generalization (eg 1 to 2/2+/5/5+, 2 to 2+/5/5+, etc. This happens because the more general form has some flexibility to be able to fit part of the noise), so we can conclude that our process performs the selection properly.

\newpage 
If we plot the fitted function against the test data we obtain the following:

```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_test_functions <- lapply (1:length(test_functions), function(i) data.frame(
  n=test_n,
  d=sapply (test_n, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(test_seqs, lapply(1:length(best_models), function(i) paste("Model ", model_ensemble[[i]]$label, " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_test_functions), line_colors=c("red"))
```

We can see how they fit perfectly. We have observed that in some occassions model 2+ was selected for model 2+ due to the noise, but in the vast majority of occasions it selected either the actual model or a more general version of it.

\newpage

## Fitting on real data

```{r, echo=FALSE, include=FALSE}
generate_models_and_measures(metric_seqs, model_ensemble);
fitted_params <- lapply(nlms, function(x) lapply(x, function(nlm) if(!is.na(nlm)[1]) coef(nlm) else c()))
best_models <- lapply(AICs, function(x) which.min(as.vector(x)))
best_coefs <- lapply(1:length(nlms), function(i) fitted_params[[i]][[ as.numeric(best_models[i]) ]])
```

Now that we have made sure our pipeline works we can proceed with the fitting of the real data. After trying to fit with both the whole dataset and the aggregated data to try to address the heteroscedasticity issue we decided to go with the whole dataset, as the fit achieved was way better compared to the one obtained with the aggregated data, mainly due to the lack of data points in the latter (and also the aggregated data still presented a fair amount of variation in its variance as $n$ increased. After fitting the models to the data we obtain the following residual standard errors:

```{r, echo=FALSE}
s <- lapply(1:length(language_list), function(i) {
      sapply(1:length(model_ensemble), function(j) {
        sqrt(deviances[[i]][[j]]/(if(!is.na(nlms[[i]][[j]][1])) df.residual(nlms[[i]][[j]]) else length(metric_seqs[[i]]$n)))
      })
})
kable(data.frame(do.call(rbind, s), row.names=language_list), col.names=sapply(model_ensemble, function(x) x$label), 'latex', booktabs=T, linesep='', align='c', caption = 'Residual Standard Errors', digits=3) %>%
  kable_styling(latex_options=c("striped", "HOLD_position"), full_width=F)
```

AICs:

```{r, echo=FALSE}
kable(data.frame(do.call(rbind, AICs), row.names=language_list), col.names=sapply(model_ensemble, function(x) x$label), format='latex', caption= 'AICs', booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options=c("striped", "scale_down", "HOLD_position"))
```

And AIC differences:

```{r, echo=FALSE}
kable(data.frame(do.call(rbind, lapply(AICs, function(x) x - min(x))), row.names=language_list), col.names=sapply(model_ensemble, function(x) x$label), format='latex', caption= 'AIC Differences',booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options=c("striped", "scale_down", "HOLD_position"))
```
It interesting to observe how despite the more general model should be able to provide a better fit, sometimes the penalization on the number of parameters imposed by the AIC leads to choosing a simpler model over the more complex one.

\newpage
By listing the parameters that provide the best fit for each model we obtain the following table:

```{r, echo=FALSE}
kable(data.frame(do.call(rbind, lapply(fitted_params, function(lang) do.call(c, lapply(2:length(lang), function(i) lang[[i]] )))), row.names=language_list), col.names=do.call(c, sapply(model_ensemble, function(x) x$params)), booktabs=T, linesep='', align='c', digits=3) %>%
  kable_styling(latex_options=c("striped", "scale_down", "HOLD_position"))
```

(We were not able to properly create the second row of headers to indicate the model, so the parameters are listed in alphabetic order and the models in increasing order). We can see how the same parameters in nested models converge to similar values as expected, while the extra parameters allow for the additional fine tuning.
\newpage

Finally, If we plot the best fitting functions (being the best the ones having the lowest AIC) we obtain the following chart on regular axes:

```{r, echo=FALSE, fig.width=16, fig.height=18}
fitted_metric_functions <- lapply (1:length(avg_seqs), function(i) data.frame(
  n=avg_seqs[[i]]$n,
  d=sapply (avg_seqs[[i]]$n, function(n)
    do.call(do.call(model_ensemble[[ best_models [[i]] ]]$f, as.list(best_coefs[[i]])), list(n)))
  )
)

plot_grid(metric_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue", 
          lines=list(fitted_metric_functions), line_colors=c("red"))
```

They do indeed present a quite good fit, despite the large amount of dispersion for large values. Notice that some flavour of the 2/5 models always happen to be the best fit, something to be expected as real languages present power-law distributions.

\newpage
By plotting the same in a log-log axes we can also observe how the functions fit the data reasonably well:

```{r, echo=FALSE, fig.width=16, fig.height=18}

plot_grid(metric_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue", 
          lines=list(fitted_metric_functions), line_colors=c("red"), log=TRUE)
```

\newpage
Now, by plotting the fitted functions against the aggregated data (recall that we used the whole data to fit and select the models):

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue",
          lines=list(fitted_metric_functions), line_colors=c("red"))
```

Surprisingly they are an excellent fit of the aggregated data, up to the point where the dispersion grows too large for an exponential function to be able to fit it closely.

\newpage
By plotting on log-log axes we can see how the good fit persists:

```{r, echo=FALSE, fig.width=16, fig.height=18}
plot_grid(avg_seqs, lapply(1:length(language_list), function(i) paste(language_list[[i]], " - Best fit: Model ", model_ensemble[[ best_models[[i]] ]]$label )), xlab='n', ylab='f(n)', color="light blue", 
          lines=list(fitted_metric_functions), line_colors=c("red"), log=TRUE)
```

It is worth noting that despite the (presumed) heteroscedasticity observed in the previous section, by fitting in the whole data we were able to obtain quite good results without doing any additional modifications to the data (results way better than obtained when fitting with only the aggregated data). Nevertheless, once the dispersion grew too large no clear trend could be observed and the quality of the fit naturally would decrease.

\newpage
# Discussion

(DONE: If there is a significant difference between the fit of the functions from null hypotheses and that of alternative hypotheses.)

From our results, we can see that the null hypotheses (model 0) have a terrible fit, whereas other alternative hypotheses have way better fits. Taking a look at the AIC results, we see how the values for model 0 are always at least two times higher than any other model, which indicates it is considerably worse than the others. Visually, we see how the null hypotheses are almost upper bounding the real data. In contrast, for example, using the power-law model (model 2, which seems to be one of the best for our data, together with model 1+ and 2+), we see how it almost matches perfectly the average for each number of vertices.
It is worth noting that when the number of vertices is high, there are fewer observations, and they have more significant variance, which means it is harder to fit. Also, this means that we are dealing with heteroscedasticity.

(TODO: If the original data satisfy the assumption of homoscedasticity of the non-linear regression methods considered here. In case that it does not hold, you should explain how you have addressed it.)

As we already explained in the section "Checking for heteroscedasticity", the original data doesn't satisfy the assumption of homoscedasticity. Instead, it clearly shows heteroscedasticity in the plots from the section mentioned before, where we can see a massive increase in the variance for higher values. 

(DONE: Discuss if the function giving the best fit gives a reasonably good fit (e.g., checking visually that the best function provides a sufficiently good fit. Remember that the best function of an ensemble is not necessarily the best in absolute terms.)

As mentioned earlier in the discussion, the best models are 1+, 2, and 2+. All those methods are different versions of the power-law model, which seems logical since we can see an almost straight line for the averages in a log-log plot. When checking them visually, we can see that they fit the real data accurately. With the aggregated data, it is even more evident that the model describes the distribution perfectly.

(DONE: The extent to which languages resemble or differ.)

From here, we can say that all the language distributions seem to follow a power-law model. Also, as we already stated in the "Properties summary" section, for most of the languages, the average number of nodes ranges from 16 to 26, except for Basque, Chinese, and Turquish, which have lower values. Regarding the mean dependency length, most languages have a value oscillating around 2, except for Chinese with a lower value (1.44), and, English and Hungarian with higher values (3.05 and 3.87 respectively). Finally, the standard deviation also is consistent at around 0.8 through most languages, except for Chinese (0.48) and Hungarian (1.78).

(DONE(?): conclusions)

To sum up, we managed to find a function of $n$ that is reasonably accurate to represent the scaling of $\langle d\rangle$ (mean length of the edges). This function is slightly different for each language, but all of them are based in a power-law model.

# Methods
## Finding the initial values
To find appropriate initial values to improve the performance of the non-linear regression, we perform a double logarithmic transformation as described in the guide for model 2. With this method, we get the following formulas:
\begin{table}[H]
\centering
    \begin{tabular}{cc}
        \toprule
        Model & Log formula \\
        \midrule
        1 & $\log\langle d\rangle = b\log(\frac{n}{2}) + 0$ \\
        2 & $\log\langle d\rangle = b\log(n) + a'$ \\
        3 & $\log\langle d\rangle = cn + a'$ \\
        4 & $\log\langle d\rangle = \log(\log(n))+ a'$ \\
        \bottomrule
    \end{tabular}
\end{table}
\textit{Note that: $a' = \log(a)$}

Then we apply a linear regression to these formulas to find the coefficients. Then we directly assign the coefficients directly (recall $a = \exp(a')$).

To calculate the models "+" version, we use the same strategy as for the regular models. The only difference is that we set the new extra value as the minimum $\langle d\rangle$ and subtract it to the target $\langle d\rangle$ before the linear regression.
