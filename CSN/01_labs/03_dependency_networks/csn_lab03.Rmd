---
title: "CSN_Lab03"
author: "Francisco Javier Jurado, Wilmer Uruchi Ticona"
date: "October 27, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
require(knitr)
opts_chunk$set(echo = TRUE)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r}
languages = read.csv('data/language_list.txt', header=FALSE)$V1
t1_header=c('Language', 'N', 'E', 'Mean Deg', 'delta')
read.csv('results/lang_summary.csv', col.names=t1_header, header=FALSE)
```



```{r include=FALSE, echo=FALSE}
# Load the igraph library and require the packages stats4 (mle) & VGAM (Riemman-zeta)
require(igraph)
require(knitr)      # This is needed for PDF making
require('stats4')
require('VGAM')
require(kableExtra) # This is needed for nice tables from R
```
# Introduction
In this lab session we study the process of determining the significance of network metrics. For this purpose we use a series of collections of global syntactic dependency trees from different languages, where vertices are words and edges indicate a syntactic dependence between two words (Based on occurrences of said dependencies in a syntactic dependency treebank). Our metric of study will be the __Closeness Centrality__ of the network's vertices, a measure of how central or how _close_ a vertex is to the other vertices in the network. The closeness centrality for a given vertex $i$ is defined as:
\[
  C_i = \frac{1}{N-1} \sum_{j=1,(i\neq j)}^N \frac{1}{d_{ij}}
\]
Where $d_{ij}$ is the geodesic distance between vertices $i$ and $j$. As we want to be able to characterize get a metric on the whole network rather than on the individual vertices, we will calculate the __Mean Closeness Centrality__, defined as:
\[
  C = \frac{1}{N} \sum_{i=1}^N C_i
\]


\end{enumerate}

\newpage
\section{Results}

\newpage
\section{Discussion}

# Methods
## Programming language choice
Due to the large amount of computations required to calculate the mean closeness centrality we have chosen to write our code in _C++_. The need to be able to define abstract data structures for the graph and to access and manipulate it efficiently led us to use this language, which allows object-oriented programming and a great performance at the cost of veing more difficult to write and debug than other, higher level languages (amongst other issues outside of the scope of this document). This report was written in RMarkdown, reading the results of the c++ programs and using them to generate the required tables.

## Data Structures
We implemented an adjacency list-based graph data structure to store and perform operations on the graphs. Each entry in the adjacency list corresponds to a vertex $v \in V$ and it contains a list of $v$'s neighbours' indexes to enable easy neighbourhood check. Because the graph is sparse the average node degree is quite low (less than 10) so we can check neighbourhood efficiently with a list scan.  
As the graphs are undirected, in order to enable fast neighbourhood checks we add the relation in both directions (vertex $u$ has $v$ in his neighbourhood list and vertex $v$ has $u$ in theirs). This effectively doubles the number of entries in the neighourhood lists, but greatly simplifies and speeds the traversal of the graph (which is the main operation we perform in this lab).  
We have also abstracted the label from the adjacency list, as it is way easier to work with integers rather than full strings. For that purpose the Graph class includes a label index, a list of size $N$ with the label at position $i$  (1-indexed) correponds to the node of id $i$. Although not strictly necessary, to allow for fast membership check we have also included a dictionary (or map) which map each word to its corresponding index.

## Data Preparation
The syntactic dependency graphs are stored as a series of \textit{.txt} files, with the (expected) number of nodes and edges in the first row and one edge per subsequent row, described as a pair of two words. The edges are only listed in one direction and in alphabetical order. To avoid intermediate storage the edges are added to the graph one at a time, adding vertices when necessary. As the graph contain loops, any loop found is not added to the graph. Malformed edges in the file are omitted too (although the vertices are added). A special mention (shootout to Amalia for finding about it) is the existence of nodes in the Arabic graph labeled with a \textit{non-breaking space}, a special kind of whitespace that was not properly parsed out when building the dependence bank. We took those into account and removed them when found.

## Null model graphs generation
In order to generate the null graphs used as null hypothesis to test the significance against we implemented the follwing procedure:
### Erdös-Rényi graph of constant N and E
For the ER graph we started from a fully disconnected graph of size E and selected pairs of vertices UAR from it. If the edge they would form  did not exist and it was well-formed (loops, multiedges...) it was added to the graph, repeating this process until the graph had E edges. In case of any ill-formed or already selected edge the values were simply discarded and two new ones were chosen. As the vertices are chosen independently from the status of the graph this re-pick does not incur in any bias.

### Switching model graph
The switching model graph is a randomized graph with the same degree sequence as the original graph. This is achieved by performing a series of switches between edges, a switch consisting in interchanging the ends of two edegs (eg the pair of edges u -> v and s -> t would result in edges u -> t and s -> v after the switching). The amount of switches to be performed is determined by $Q \cdot E$, $E$ being the number of edges and $Q$ a scale parameter to try to ensure that all edges have been selected for a switch at least once. In order to choose $Q$ we use the coupon collector's problem, which poses the question of what is the expected number of coupons from a collection of $N$ coupons a collector should buy to ensure that he has collected them all, given that every time she buys a coupon she get one of the of the possible $N$ coupons at random. The mathematical analysis of this problem reveals that this number is $\Theta(n log (n))$, and therefore we choose $Q = log(n)$.  
In our implementation we initially choose valid edges by first selecting a vertex at random and then selecting a second vertex from its neighbours list. If an isolated vertex is chosen the switch is marked as failed and not performed, as it would alter the degree sequence. Failed switched are still counted for the total $Q \cdot E$. There are two other main situations in which a switch can fail without changing the degree sequence:
 - __Edges with the form u -> v, s -> u are chosen__: A loop in vertex $u$ is generated after the switch.
 - __Given two edges u -> v and s -> t, there is an edge u -> t__: A multiedge would be formed between $u$ and $t$ after the switch.
Any edges of this kind found are marked as invalid and counted towards the overall edge count.  
There are several instances for which the switching is trivial (the edges stay as before), mainly when the same edge is chosen twice or the vertex at a given end in both edges is the same. When this happens it does not incur into an invalid switch, but it can be skipped to avoid unnecesary computation.

## $C_i$ computation

## Exact optimizations

## Approximate optimizations



