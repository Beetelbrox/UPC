---
title: "Complex & Social Networks: Lab 02 Report"
author:
  - Francisco Javier Jurado Moreno
  - Sergio Mosquera Dopico
output: pdf_document
---

```{r setup, include=FALSE}
require(knitr)
opts_chunk$set(echo = TRUE)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

#```{r set-options, include=FALSE}
#options(width=80)
#```

```{r include=FALSE, echo=FALSE}
# Load the igraph library and require the packages stats4 (mle) & VGAM (Riemman-zeta)
require(igraph)
require(knitr)
require('stats4')
require('VGAM')
require(kableExtra)

# Define the minus log likelihood functions for the different probaiblity mass functions to be passed to the optimizer
models <- c('Poisson', 'Geometric', 'Zeta_2', 'Zeta', 'Right-truncated Zeta', 'Altmann')
params <- c("lambda", 'q', 'gamma_1', 'gamma_2', 'k_max', 'gamma', 'delta', 'k_max_2')
num_params <- c(1, 1, 0, 1, 2, 2)

H <- function(gamma, k_max) { sum(sapply(seq(from=1, to=k_max, by=1), function(k) k^(-gamma))) }
c_f <- function(gamma, delta, k_max) { 1/sum(sapply(seq(from=1, to=k_max, by=1), function(k) (k^(-gamma))*exp(-delta*k))) }

pmfs <- c(
  # Displaced Poisson
  function(k, lambda) { ((lambda^k)*exp(-lambda))/(factorial(k)*(1-exp(-lambda))) },

  # Displaced geometric
  function(k, q) { (1-q)^(k-1)*q },

  # Zeta with gamma=2
  function(k) { (k^(-2))/zeta(2) },

  # Zeta
  function(k, gamma) { (k^(-gamma))/zeta(gamma) },

  # Right-truncated Zeta
  function(k, gamma, k_max) { (k^(-gamma))/H(gamma, k_max) },

  # Altmann
  function(k, gamma, delta, k_max) { c_f(gamma, delta, k_max)*(k^(-gamma))*exp(-delta*k) }
)

generate_minus_log_likelihoods <- function(stats) {
  c(
    # Displaced Poisson
    function(lambda){
      stats$N*(lambda + log(1 - exp(-lambda))) + stats$C - stats$M*log(lambda)
    },

    # Displaced Geometric
    function (q) { (stats$N - stats$M)*log(1-q) - stats$N*log(q) },

    # Zeta with gamma=2
    function() { 2*stats$M_prime + stats$N*log((pi^2)/6) },

    # Zeta
    function(gamma) { stats$M_prime*gamma + stats$N*log(zeta(gamma))},

    # Right-truncated Zeta
    function(gamma, k_max){
      gamma*stats$M_prime + stats$N*log(H(gamma, k_max))
    },

    # Altmann function
    function(gamma, delta, k_max) { gamma*stats$M_prime + delta*stats$M - stats$N*log( c_f(gamma, delta, k_max ) ) }
  )
}

start_parameters <- c(
  function(stats) { list( lambda=stats$M/stats$N ) },                  # Displaced Poisson
  function(stats) { list( q=stats$N/stats$M ) },                       # Displaced Geometric
  NA,                                                                  # Zeta w/ gamma=2
  function(stats) { list( gamma=2 )},                                  # Zeta
  function(stats) { list( gamma=2, k_max=stats$max )},                 # Right-truncated Zeta
  function(stats) { list(gamma=2, delta=0.0000001, k_max=stats$max )}  # Altmann
)

lower_bounds <- c(
  function(stats) { c(1.0000001) },
  function(stats) { c(0.0000001) },
  NA,
  function(stats) { c(gamma=1.0000001) },
  function(stats) { c(gamma=1.0000001, k_max=stats$max-0.0000001) }, # Substratc 1e-6 to get rid of the 'NaNs produced' issue
  function(stats) { c(gamma=1.0000001, delta=0.0000001, k_max=stats$max-0.0000001) }
)

upper_bounds <- c(
  function(stats) { NA },
  function(stats) { c(0.9999999) },
  NA,
  function(stats) { NA },
  function(stats) { c(NA, k_max=2*stats$max) }, # 2*max is a reasonably low bound
  function(stats) { c(NA, 0.5, k_max=stats$max*2) }
)

# Function to calculate the likelihoods given a set of statistics
calculate_likelihoods <- function(stats) {
  minus_log_likelihoods <- generate_minus_log_likelihoods(stats)
  lapply(seq(from=1, to=length(models), by=1), function(i) {
    if(num_params[i] > 0) {
          mle(minus_log_likelihoods[[i]],
              start = start_parameters[[i]](stats),
              method = "L-BFGS-B",
              lower = lower_bounds[[i]](stats),
              upper = upper_bounds[[i]](stats)
          )
    }})
}

# Function to calculate the AIC from an mle
# calculated for some distribution
calculate_AIC <- function(m2logL, K, N){
  m2logL + 2*K*N/(N-K-1)
}

generate_tables <- function(in_degree_sequences) {
  lang_stats <<- lapply(in_degree_sequences, function(x) list(
                N=length(x),
                M=sum(x),
                max=max(x),
                M_prime=sum(log(x)),
                C=sum(sapply(seq(from=1, to=length(x), by=1), function(i) sum(sapply(seq(from=1, to=x[i], by=1), function(j) log(j)))))))

  coef_estimates <<- lapply(lang_stats, function(x) lapply(calculate_likelihoods(x), function(l) {attributes(summary(l))$coef[,1]}))

  AICs <<- lapply(seq(from=1, to=length(lang_stats), by=1), function(l) {
    minus_log_likelihoods <- generate_minus_log_likelihoods(lang_stats[[l]])
    unlist(lapply(seq(from=1, to=length(models), by=1), function(i) {
      calculate_AIC(2*do.call(minus_log_likelihoods[[i]], as.list(coef_estimates[[l]][[i]])), num_params[i], lang_stats[[l]]$N)
    }))
  })
}

run_test <- function() {
  test_prefix_geom <- 'sample_of_geometric_with_parameter_'
  test_prefix_zeta <- 'sample_of_zeta_with_parameter_'
  test_geom_param_values <- c(0.05, 0.1, 0.2, 0.4, 0.8)
  test_zeta_param_values <- c(2, 2.5, 3, 3.5)
  test_param_values <- c(test_geom_param_values, test_zeta_param_values)

  test_names_geom <- sapply(test_geom_param_values, function(x)
    paste('Geometric [q = ', x,']', sep=''))
  test_names_zeta <- sapply(test_zeta_param_values, function(x)
    paste('Zeta [gamma = ', x,']', sep=''))
  test_names <- c(test_names_geom, test_names_zeta)

  test_filenames_geom <- sapply(test_geom_param_values,
          function(x) paste('test/', test_prefix_geom, x, '.txt', sep=''))
  test_filenames_zeta <- sapply(test_zeta_param_values,
          function(x) paste('test/', test_prefix_zeta, x, '.txt', sep=''))
  test_filenames <- c(test_filenames_geom, test_filenames_zeta)

  in_degree_sequences <<- sapply(test_filenames, function(x) read.table(x, header=FALSE))
  output_table_labels <<- test_names

  generate_tables(in_degree_sequences)

}

run_real <- function() {
  # Read the list of languages from file
  languages = read.table("list.txt",
                header = TRUE,
                as.is = c("language","file")
              )

  # Read the language files from disk and store them in memory so we don't have to do so multiple times
  in_degree_sequences <<- sapply(languages$file, function(x) read.table(x, header = FALSE))
  output_table_labels <<- languages$language

  generate_tables(in_degree_sequences)
}
```

#Introduction

The goal of this lab session is to practice on information theoretic model selection for the degree distribution of global syntactic dependency networks. We are required to perform a deep analysis over the \textit{in-degree-sequence} dataset. This analysis would be divided in four main phases:

\begin{enumerate}

\item \textbf{Study the basic properties than can be obtained from the given degree sequence}  
As a first step we can directly extract key properties directly from the datasets like their size (\textbf{N}), maximum node degree or mean degree (as it is done in the file \textit{summary\_table.R}). These will be used in the model parameter estimation.

\item \textbf{Estimation of the best parameters for different degree distributions using Maximum Likelihood Estimation (mle)}  
Once we have extracted the basic properties from the sequence we would like to analyze its behavior, i.e. find in an ensemble of degree distribution the one which better fits the real data. To achieve this we make use of R's \textit{mle} method which performs maximum likelihood estimation of parmeters given a log-likelihood function and a set of starting and bound conditions. The parameter values obtained allow to select for each dataset and family of functions which one fits the data best.

\item \textbf{Selection of the best model according to Akaike Information Criterion (AIC)}
After estimating which are the values for the parameters that provide the best fit for each distribution and language we finally have to select which one models best the real data. To do so we use the Akaike Information Criterion (AIC), which provides a measure of how well a model fits the data with a regularization for the amount of parameters. Once the AIC is calculated for all models on a given dataset, we select the one with the lowest score as the best-fitting model. In order to be able to compare the quality of fit between models we use the AIC difference, which is the difference in score between the best-fitting model and the rest. This allows us to see how close are two models on their ability to fit data.
\end{enumerate}

\newpage
\section{Results}

## On code correctness
```{r, echo=FALSE, include=FALSE}
run_test()
```

As suggested in the project statement, before proceeding with the real data we want to make sure our script works properly by running it on datasets generated following known distributions. For that purpose we used the geometric and zeta distribution datasets lecturer, omitting the one for $\gamma=1.5$ as it is a problematic instance that made the _mle_ crash. The table with the model selection obtained (models for which $AIC_{diff}=0$) for each one of the datasets can be found below:

```{r, echo=FALSE}
delta_AIC_df <- data.frame(do.call(rbind, lapply(AICs, function(x) sapply(x, function(y) y - min(x)))), row.names=output_table_labels)
kable(delta_AIC_df,  col.names=models, booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```
We can see that our script correclty selects the distribution used to generate the dataset, choosing between the regular and right-truncated version of the Zeta function for the zeta-generated datasets depending on which one provides the best fit (%EXPLANATION HERE%).  
Now want to check if the parameters of the selected models were properly estimated:
```{r, echo=FALSE}
parameters_df <- data.frame(do.call(rbind, lapply(coef_estimates, unlist)), row.names=output_table_labels)

kable(parameters_df, col.names=params, booktabs=T, linesep='', align='c', digits=3) %>%
  kable_styling(latex_options="striped", full_width=F)
```
By checking the cells corresponding to the parameters of the selected models we can see that the estimation matches the real value quite closely. We can conclude then that the pipeline works properly.

\newpage
## Model selection on the in-degree data
```{r, echo=FALSE, include=FALSE}
run_real()
```

Before starting with the model selection for the real data let's plot the normalized degree spectrum in log-log scalef or all degree sequences:

```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(in_degree_sequences)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(in_degree_sequences), by=1)) {
  spectrum = as.data.frame(table(in_degree_sequences[[i]]))
  plot(x=as.numeric(spectrum$Var1),
       y=as.numeric(spectrum$Freq)/lang_stats[[i]]$N,
       type='o',
       main=output_table_labels[[i]],
       log='xy',
       xlab='k',
       ylab='n(k)/N',
       col='blue')
}
```

As seen in the lectures, the fact that for all sequences the degree spectrum draws an almost straight line in a log-log plot gives us some good intuition that they might follow a power-law, although this linearity gets worse as $k$ increases. This matches with the observation that the in-degree tends to be more power-law than the out-degree, and we can expect the nested Zeta variants and Altmann distributions to be a better fit to the data than the Poisson and geometric ones.

\newpage
## Degree sequence statistical properties
We can now generate the summary of the properties of the degree sequnces:

```{r, echo=FALSE}
# Apply generate_language_summary to every entry in the list of language in-degrees to generate the summary for every language. Use do.call to pass each summary to rbind as a parameter, that way the vectors are concatenated vertically
summary_df <- data.frame(do.call(rbind, lapply(lang_stats, function(x) c(x$N, x$max, x$M/x$N, x$N/x$M))), row.names=output_table_labels)

kable(summary_df, col.names=c('N', 'Max Degree', 'M/N', 'N/M'), booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```

Although the plots are quite similar to one another, the values of the properties of the different in-degree sequences vary significantly, from the number of nodes to the mean or maximum degree. 

## Model parameter estimation
Using the previously calculated parameters and the log-likelihood functions of each model with R's \textit{mle} function we can obtain the parameters which give the best fit for each function and in-degree sequence:

```{r, echo=FALSE}
parameters_df <- data.frame(do.call(rbind, lapply(coef_estimates, unlist)), row.names=output_table_labels)

kable(parameters_df, col.names=params, booktabs=T, linesep='', align='c', digits=3) %>%
  kable_styling(latex_options="striped", full_width=F)
```
It is worth noting that, for each degree sequence, the estimated values of $k_{max}$ both for the right-truncated zeta and Altmann distributions correspond to the maximum degree of the sequence. This behaviour is appropriate given the existing data, as the \textit{mle} finds out that the best value for the maximum degree is that of the maximum degree on the real data. The fact that it's the same for both distributions is also to be expected, as the Altmann distribution is a more general case of the right-truncated zeta (for $\delta=0$), that is, it is another nesting level of the zeta functions.

\newpage
## Model Selection
Using the previous parameter values we can obtain the corresponding log-likelihood value and with it the $AIC_{diff}$ values:

```{r, echo=FALSE}
delta_AIC_df <- data.frame(do.call(rbind, lapply(AICs, function(x) sapply(x, function(y) y - min(x)))), row.names=output_table_labels)
kable(delta_AIC_df,  col.names=models, booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```
We can see how the Altmann function is the better fit for all languages but the Hungarian and Turkish, which seem to be better fitted by a Zeta distribution. It is interesting to note that for Greek, Hungarian, Italian and Turkish the $AIC_{diff}$ values are the same for the Altmann and right-truncated zeta distribution. This happens because, as we mentioned before, the Altmann is a more general case of the right-truncated zeta. If we check the value of $\delta$ on the previous table we can see that for those datasets it equals 0, making the two distributions equal (as the rest of the distributions become identical if $\delta=0$, the estimator gives the same value to the rest of the parameters too).

\newpage
We can now plot the best-fitting model against the original data to be able to visually inspect the quality of the fit:
```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(in_degree_sequences)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(in_degree_sequences), by=1)) {
  best <- as.numeric(which.min(delta_AIC_df[i,]))
  best_params <- as.list(coef_estimates[[i]][[best]])
  spectrum <- as.data.frame(table(in_degree_sequences[[i]]))
  best_fit <- sapply(as.numeric(spectrum$Var1), function(x) do.call(pmfs[[best]], c(x, best_params)))
  plot(x=as.numeric(spectrum$Var1),
       y=as.numeric(spectrum$Freq)/lang_stats[[i]]$N,
       type='o',
       main=paste(output_table_labels[[i]], ' - Best fit: ', models[best], sep=''),
       log='xy',
       xlab='k',
       ylab='n(k)/N',
       col='blue')

  lines(x=as.numeric(spectrum$Var1),
       y=best_fit,
       type='o',
       col='red')

  legend("topright", legend=c("Real data", models[best]),
       col=c('blue', 'red'), lty=1, pch=1, cex=1.8)
}
```
The fit seems to be quite good for all functions, 

\newpage
\section{Discussion}

In fact, there is a significant difference between the fit using zeta models and the null ones. We just need to take into account the results shown in the table that details the ∆AIC values for the intial data. There is a tendency among almost every language, such that the model that fitted the best was the right-truncated
zeta before the inclusion of the Altmann function. For all languages zeta functions behavior was significantly better than the null models, specially for the right-truncated zeta which returned the AIC best in 8 out of 10 languages.
After adding the Altmann function, this result has changed drastically. We were able to check that this new function was the best taking into account the AIC dif f value for 8 out 10 languages. The other 2 best results correspond to the zeta distribution. With these results it is demonstrated that Altmann is the best model
even thinking of groups of distributions, for instance it is better in all the cases than the null models, and in
most of them than the zeta functions. Also it is important to remark that even when it does not return the
best fit, the result is still very close to the best one.

Exponential vs heavy tail

Taking a look

% Summary de los resultados y nuestra interpretación particular de los mismos. Principales temas a tratar:

% - If there is any significant difference between the fit of the distributions from null models and those of the zeta family.

In fact, there is a significant difference between the fit using zeta models and the null ones. We just need to take into account the results shown in Table \ref{table:aic}, which details the $\Delta$AIC values for the intial data. There is a tendency among almost every language, such that the model that fits the best is the right-truncated zeta.


% - If the distribution giving the best fit gives a reasonably good fit (e.g.). Remember that the best function of an ensemble is not necessarily the best in absolute terms.

% - The extent to which languages resemble or differ.

% - Conclusions

\newpage
\section{Methods}
In order to minimize the amount of repeated code and to enable an easy way to add more functions without having to modify the main body of the script, for this project we have implemented our solution using functional programming. This has allowed us to define a pipeline which takes a list of probability mass functions, minus log likelihoods, parameters and bounds (which can be read from file and easily expanded) and performs the fitting and model selection, returning the data structures necesary to generate the tables and the plots.

For our additional, better fitting distribution we have used a modified version of the Altmann function, as proposed in the project statement and originally by G. Altmann as a mathematical statement of the Menzerath-Altmann law. The difference between our version and the one in the statement lies in using a third parameter $k_max$ instead of $N$ to calculate $c$ in the same spirit as the Right-Truncated Zeta distribution. This way the optimizer can (and will) adjust $k_{max}$ accordingly to provide a better value for $c$. The Altmann function has the form:
\[p(k)=ck^{-\gamma}e^{-\delta k}\]
for $1 \leq k \leq N$ and 0 otherwise, $c$ being:
\[\frac{1}{\sum^{k_{max}}_{k=1} k^{-\gamma}e^{-\delta k} } \]

Its log-likelihood can be derived as follows:
\[ \sum^N_{k=1} log(ck^{-\gamma}e^{-\delta k}) = \sum^N_{k=1} log(c) -\gamma \cdot log(k) -\delta \cdot k = N \cdot log(c) - \gamma \cdot M' -\delta \cdot M \]

Regarding to the starting values and bounds for the mle we have tried to stick to the ones proposed in the statement as much as possible. As such, we selected for our starting values $\lambda=M/N$ (Poisson), $q=N/M$ (Geometric) and $\gamma=2$ (Zeta & Altmann distributions), with $\lambda > 1$, $0 < q < 1$ and $\gamma > 1$. For $k_{max}$ (both in the right-truncated zeta & Altmann) we gave it an inital value equal to the maximum degree in the list. In order for the optimizer to select that value we . This allows the optimizer to select the maximum degree value for $k_{max}$, which is the best estimation possible (the real value). Because the mle crashes without an upper bound for $k_{max}$, we set it to twice the maximum degree in order to provide some space for the optimizer to perform the search. Finally, for the Altmann function's $\delta$ we made it so $0 < \delta < 1$ and gave it an initial value of 0.5 after a fair amount of trial and error.

\section*{Appendix}

Akaike Information Criterion for small samples:

\begin{equation}\label{eq:aic}
    AIC_c = -2\mathcal{L} + 2K\frac{N}{N-K-1}
\end{equation}
