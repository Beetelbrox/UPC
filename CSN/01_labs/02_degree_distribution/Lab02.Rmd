---
title: "Complex & Social Networks: Lab 02 Report"
author:
  - Francisco Javier Jurado Moreno
  - Sergio Mosquera Dopico
output: pdf_document
---

```{r setup, include=FALSE}
require(knitr)
opts_chunk$set(echo = TRUE)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

#Introduction

The goal of this lab session is to practice on information theoretic model selection for the degree distribution of global syntactic dependency networks. We are required to carry out a deep analysis over the \textit{in-degree-sequence} dataset. This analysis would be divided in four main phases:

\begin{enumerate}

\item \textbf{Study the basic properties than can be obtained from the given degree sequence.} We can obtain direct measurements from this dataset (as it is done in the file \textit{summary\_table.R}) as they may be the number of nodes \textbf{N} which is computed by taking the number of rows in the dataset, the sum of the degrees \textbf{M}, the mean degree \textbf{M/N}, ...

\item \textbf{Estimation of the best parameters for different degree distributions using Maximum Likelihood Estimation (mle).} Once we have extracted the basic knowledge from the sequence, we want to analyze its behavior, i.e. find the degree distribution providing the best fit with respect to it. To do this, we need to maximize the log-likelihood function for each distribution (Poisson, geometric, zeta with gamma=2, zeta and right truncated zeta). Luckily for us, R provides a library named \textit{stats4} with some functions to ease this task of obtaining the best parameters. The most important one is the \textit{mle} function that performs a calculation of the maximum likelihood estimation getting as input a log-likelihood function from a given distribution, some initial parameters for that distribution and (in some cases) upper and/or lower bounds. The output of this function returns, among others, the value of the distribution parameters that would fit the best the model.

\item \textbf{Selection of the best model according to Akaike Information Criterion (AIC).} After obtaining the best values for all the parameters using log likelihood, we have to decide which is the best model, i.e. the one that fits the best the original data. That is decided by calculating the AIC value for each of the distributions and each of the languages. Finally, we calculate the delta of the AICs for each of the languages so we can state which is the best in most of the cases. If we take into account Formula:

\begin{equation}
    AIC_c = -2\mathcal{L} + 2K\frac{N}{N-K-1}
\end{equation}

We already know all the needed values from previous steps but $-2\mathcal{L}$. Among all the different values that can be extracted from the \textit{mle} results, one of them is $-2\mathcal{L}$. The last step of this phase would consist in including a new probability distribution that is more likely to return better results than the other models. We are using the Menzerath-Altmann law formulation:

\begin{equation}
    p(k) = ck^{-\gamma}e^{-\delta k}
\end{equation}

\item \textbf{Test our implementation with samples from which we already know the distribution.} The problem we have with the initial dataset is that the distribution the sequence follows is unknown, so we cannot ensure the values we are obtaining are correct or not. To solve this problem, we are provided with a new dataset of samples from discrete distributions, such that each sample explains beforehand which is the distribution it is following and with which parameters. Once we know that, we should run the same experiments as before but with this new dataset and check whether the parameters we are obtaining using \textit{mle} match the ones used by the sample.
\end{enumerate}

#Results

#Discussion

#Methods

```{r cars, include=FALSE}
# Load the igraph library and require the packages stats4 (mle) & VGAM (Riemman-zeta)
require(igraph)
require(knitr)
require('stats4')
require('VGAM')
require(kableExtra)
```

```{r, echo=FALSE}
# Define the minus log likelihood functions for the different probaiblity mass functions to be passed to the optimizer
models <- c('Poisson', 'Geometric', 'Zeta_2', 'Zeta', 'Right-truncated Zeta', 'Altmann')
params <- c("lambda", 'q', 'gamma_1', 'gamma_2', 'k_max', 'gamma', 'delta')
num_params <- c(1, 1, 0, 1, 2, 2)

H <- function(gamma, k_max) { sum(sapply(seq(from=1, to=k_max, by=1), function(k) k^(-gamma))) }
c_f <- function(N, gamma, delta) { 1/sum(sapply(seq(from=1, to=N, by=1), function(k) (k^(-gamma))*exp(-delta*k))) }

generate_minus_log_likelihoods <- function(stats) {
  c(
    # Displaced Poisson
    function (lambda) { stats$N*(lambda + log(1 - exp(-lambda))) + stats$C - stats$M*log(lambda) },

    # Displaced Geometric
    function (q) { (stats$N - stats$M)*log(1-q) - stats$N*log(q) },

    # Zeta with gamma=2
    function() { 2*stats$M_prime + stats$N*log((pi^2)/6) },

    # Zeta
    function(gamma) { stats$M_prime*gamma + stats$N*log(zeta(gamma))},

    # Right-truncated Zeta
    function (gamma, k_max) { gamma*stats$M_prime + stats$N*log(H(gamma, k_max)) },

    # Altmann function
    function(gamma, delta) { gamma*stats$M_prime + delta*stats$M - stats$N*log( c_f(stats$N, gamma, delta ) ) }
  )
}

start_parameters <- c(
  function(stats) { list( lambda=stats$M/stats$N ) },
  function(stats) { list( q=stats$N/stats$M ) },
  NA,
  function(stats) { list( gamma=2 )},
  function(stats) { list( gamma=2, k_max=stats$max )},
  function(stats) { list(gamma=1, delta=0 )}
)

lower_bounds <- c(
  function(stats) { c(1.0000001) },
  function(stats) { c(0.0000001) },
  NA,
  function(stats) { c(1.0000001) },
  function(stats) { c(gamma=1.0000001, k_max=stats$max-0.0000001) }, # Substratc 1e-6 to get rid of the 'NaNs produced' issue
  function(stats) { c(1.0000001, 0.0000001) }
)

upper_bounds <- c(
  function(stats) { NA },
  function(stats) { c(0.9999999) },
  NA,
  function(stats) { NA },
  function(stats) { c(NA, k_max=2*stats$max) }, # 2*max is a reasonably low bound
  function(stats) { c(NA, 0.9999999) }
)

# Function to calculate the likelihoods given a set of statistics
calculate_likelihoods <- function(stats) {
  minus_log_likelihoods <- generate_minus_log_likelihoods(stats)
  lapply(seq(from=1, to=length(models), by=1), function(i) {
    if(num_params[i] > 0) {
      print(i)
          mle(minus_log_likelihoods[[i]],
              start = start_parameters[[i]](stats),
              method = "L-BFGS-B",
              lower = lower_bounds[[i]](stats),
              upper = upper_bounds[[i]](stats)
          )
    }})
}

calculate_AIC <- function(m2logL, K, N){
  m2logL + 2*K*N/(N-K-1)
}
```

```{r}

###################################################################################################################################
# TEST SWITCH - Make 'test=TRUE' to run
###################################################################################################################################
test = FALSE

if (test) {
  test_prefix_geom <- 'sample_of_geometric_with_parameter_'
  test_prefix_zeta <- 'sample_of_zeta_with_parameter_'
  test_geom_param_values <- c(0.05, 0.1, 0.2, 0.4, 0.8)
  test_zeta_param_values <- c(2, 2.5, 3, 3.5)
  test_param_values <- c(test_geom_param_values, test_zeta_param_values)
  
  test_names_geom <- sapply(test_geom_param_values, function(x) paste('Geometric [q = ', x,']', sep=''))
  test_names_zeta <- sapply(test_zeta_param_values, function(x) paste('Zeta [gamma = ', x,']', sep=''))
  test_names <- c(test_names_geom, test_names_zeta)
  
  test_filenames_geom <- sapply(test_geom_param_values, function(x) paste('test/',test_prefix_geom, x, '.txt', sep=''))
  test_filenames_zeta <- sapply(test_zeta_param_values, function(x) paste('test/',test_prefix_zeta, x, '.txt', sep=''))
  test_filenames <- c(test_filenames_geom, test_filenames_zeta)
  
  in_degree_sequences <- sapply(test_filenames, function(x) read.table(x, header=FALSE))
  output_table_labels=test_names
}else{
  # Read the list of languages from file
  languages = read.table("list.txt",
                header = TRUE,               # this is to indicate the first line of the file contains the names of the columns instead of the real data
                as.is = c("language","file") # this is need to have the cells treated as real strings and not as categorial data.
              )
  
  # Read the language files from disk and store them in memory so we don't have to do so multiple times
  in_degree_sequences <- vector('list', nrow(languages))
  for (i in seq(from=1, to=nrow(languages))){
    in_degree_sequences[i] <- read.table(languages$file[i], header = FALSE)
  }
  output_table_labels=languages$language
}

lang_stats <- lapply(in_degree_sequences, function(x) list(
                N=length(x),
                M=sum(x),
                max=max(x),
                M_prime=sum(log(x)),
                C=sum(sapply(seq(from=1, to=length(x), by=1), function(i) sum(sapply(seq(from=1, to=x[i], by=1), function(j) log(j)))))))

coef_estimates <- lapply(lang_stats, function(x) lapply(calculate_likelihoods(x), function(l) attributes(summary(l))$coef[,1]))

AICs <- lapply(seq(from=1, to=length(lang_stats), by=1), function(l) {
  minus_log_likelihoods <- generate_minus_log_likelihoods(lang_stats[[l]])
  unlist(lapply(seq(from=1, to=length(models), by=1), function(i) {
    calculate_AIC(2*do.call(minus_log_likelihoods[[i]], as.list(coef_estimates[[l]][[i]])), num_params[i], lang_stats[[l]]$N)
  }))
})
```

```{r, echo=FALSE}
# Apply generate_language_summary to every entry in the list of language in-degrees to generate the summary for every language. Use do.call to pass each summary to rbind as a parameter, that way the vectors are concatenated vertically
summary_df <- data.frame(do.call(rbind, lapply(lang_stats, function(x) c(x$N, x$max, x$M/x$N, x$N/x$M))), row.names=output_table_labels)

kable(summary_df, col.names=c('N', 'Max Degree', 'M/N', 'N/M'), booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options = "striped", full_width=T)
```

```{r}
parameters_df <- data.frame(do.call(rbind, lapply(coef_estimates, unlist)), row.names=output_table_labels)

kable(parameters_df,  col.names=params, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options = "striped", full_width=T)
```



```{r}
delta_AIC_df <- data.frame(do.call(rbind, lapply(AICs, function(x) sapply(x, function(y) y - min(x)))), row.names=output_table_labels)
kable(delta_AIC_df,  col.names=models, booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options = "striped", full_width=T)
```
