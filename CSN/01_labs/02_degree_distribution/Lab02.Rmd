---
title: "Complex & Social Networks: Lab 02 Report"
author:
  - Francisco Javier Jurado Moreno
  - Sergio Mosquera Dopico
output: pdf_document
---

```{r setup, include=FALSE}
require(knitr)
opts_chunk$set(echo = TRUE)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

```{r include=FALSE, echo=FALSE}
# Load the igraph library and require the packages stats4 (mle) & VGAM (Riemman-zeta)
require(igraph)
require(knitr)      # This is needed for PDF making
require('stats4')
require('VGAM')
require(kableExtra) # This is needed for nice tables from R

#########################################################################################################
# OPTIMIZER INPUTS DEFINITION
# All inputs for a given distribution are aligned by index over the different structures
#########################################################################################################

# Define the arrays containing the names of the models, the parameter names and the amount of parameters of each function
models <- c('Poisson', 'Geometric', 'Zeta_2', 'Zeta', 'Right-truncated Zeta', 'Altmann')
params <- c("lambda", 'q', 'gamma_1', 'gamma_2', 'k_max', 'gamma', 'delta', 'k_max_2')
num_params <- c(1, 1, 0, 1, 2, 3)

# Function to calculate the Harmonic number and the c value
H <- function(gamma, k_max) { sum(sapply(seq(from=1, to=k_max, by=1), function(k) k^(-gamma))) }
c_f <- function(gamma, delta, k_max) { 1/sum(sapply(seq(from=1, to=k_max, by=1), function(k) (k^(-gamma))*exp(-delta*k))) }

# List of probabiliy mass functions
pmfs <- c(
  # Displaced Poisson
  function(k, lambda) { ((lambda^k)*exp(-lambda))/(factorial(k)*(1-exp(-lambda))) },

  # Displaced geometric
  function(k, q) { (1-q)^(k-1)*q },

  # Zeta with gamma=2
  function(k) { (k^(-2))/zeta(2) },

  # Zeta
  function(k, gamma) { (k^(-gamma))/zeta(gamma) },

  # Right-truncated Zeta
  function(k, gamma, k_max) { (k^(-gamma))/H(gamma, k_max) },

  # Altmann
  function(k, gamma, delta, k_max) { c_f(gamma, delta, k_max)*(k^(-gamma))*exp(-delta*k) }
)

# Function to generate a list of maximum likelihood given a set of sequence statistics
generate_minus_log_likelihoods <- function(stats) {
  c(
    # Displaced Poisson
    function(lambda){ stats$N*(lambda + log(1 - exp(-lambda))) + stats$C - stats$M*log(lambda) },

    # Displaced Geometric
    function (q) { (stats$N - stats$M)*log(1-q) - stats$N*log(q) },

    # Zeta with gamma=2
    function() { 2*stats$M_prime + stats$N*log((pi^2)/6) },

    # Zeta
    function(gamma) { stats$M_prime*gamma + stats$N*log(zeta(gamma)) },

    # Right-truncated Zeta
    function(gamma, k_max){ gamma*stats$M_prime + stats$N*log(H(gamma, k_max)) },

    # Altmann function
    function(gamma, delta, k_max) { gamma*stats$M_prime + delta*stats$M - stats$N*log( c_f(gamma, delta, k_max ) ) }
  )
}

# List of functions to generate the starting values given a set of stats
start_parameters <- c(
  function(stats) { list( lambda=stats$M/stats$N ) },                  # Displaced Poisson
  function(stats) { list( q=stats$N/stats$M ) },                       # Displaced Geometric
  NA,                                                                  # Zeta w/ gamma=2
  function(stats) { list( gamma=2 )},                                  # Zeta
  function(stats) { list( gamma=2, k_max=stats$max )},                 # Right-truncated Zeta
  function(stats) { list(gamma=2, delta=0.0000001, k_max=stats$max )}  # Altmann
)

# List of functions to generate the lower bound values given a set of stats
lower_bounds <- c(
  function(stats) { c(1.0000001) },
  function(stats) { c(0.0000001) },
  NA,
  function(stats) { c(gamma=1.0000001) },
  function(stats) { c(gamma=1.0000001, k_max=stats$max-0.0000001) }, # Substratc 1e-6 to get rid of the 'NaNs produced' issue
  function(stats) { c(gamma=1.0000001, delta=0.0000001, k_max=stats$max-0.0000001) }
)

# List of functions to generate the upper bound values given a set of stats
upper_bounds <- c(
  function(stats) { NA },
  function(stats) { c(0.9999999) },
  NA,
  function(stats) { NA },
  function(stats) { c(NA, k_max=2*stats$max) }, # 2*max is a reasonably low bound
  function(stats) { c(NA, 0.5, k_max=stats$max*2) }  # 0.5 makes the mle not crash
)

# Function to calculate the likelihoods given a set of statistics - This is functional and iterates over the set of distribution, so every input has
# to be passed as a function depending on stats
calculate_likelihoods <- function(stats) {
  minus_log_likelihoods <- generate_minus_log_likelihoods(stats)
  lapply(seq(from=1, to=length(models), by=1), function(i) {
    if(num_params[i] > 0) {
          mle(minus_log_likelihoods[[i]],
              start = start_parameters[[i]](stats),
              method = "L-BFGS-B",
              lower = lower_bounds[[i]](stats),
              upper = upper_bounds[[i]](stats)
          )
    }})
}

#########################################################################################################
# FUNCTIONS
#########################################################################################################

# Function to calculate the AIC from an mle
# calculated for some distribution
calculate_AIC <- function(m2logL, K, N){
  m2logL + 2*K*N/(N-K-1)
}

# Generate all the data structures that will go into the global scope and will be used to generate the tables and the plots
generate_tables <- function(in_degree_sequences) {
  # Iterates over the sequence distributions and generates the statistics for all of them
  lang_stats <<- lapply(in_degree_sequences, function(x) list(
                N=length(x),
                M=sum(x),
                max=max(x),
                M_prime=sum(log(x)),
                C=sum(sapply(seq(from=1, to=length(x), by=1), function(i) sum(sapply(seq(from=1, to=x[i], by=1), function(j) log(j)))))))

  # Iterates over the sequence distributions and models and estimates the best parameters for all of them using the language statistics
  # Returns: List of lists
  coef_estimates <<- lapply(lang_stats, function(x) lapply(calculate_likelihoods(x), function(l) {attributes(summary(l))$coef[,1]}))

  # Iterates over the sequence distributions and calculates the AICs from the minus log-likelihoods. It calculates them from scratch rather than pulling them 
  # out from the mle output to have an uniform interface and be able to use the function for the zeta with fixed gamma
  AICs <<- lapply(seq(from=1, to=length(lang_stats), by=1), function(l) {
    minus_log_likelihoods <- generate_minus_log_likelihoods(lang_stats[[l]])
    unlist(lapply(seq(from=1, to=length(models), by=1), function(i) {
      calculate_AIC(2*do.call(minus_log_likelihoods[[i]], as.list(coef_estimates[[l]][[i]])), num_params[i], lang_stats[[l]]$N)
    }))
  })
}

# Function to read the test data and run it throuh the pipeline
run_test <- function() {
  # Generate the file names
  test_prefix_geom <- 'sample_of_geometric_with_parameter_'
  test_prefix_zeta <- 'sample_of_zeta_with_parameter_'
  test_geom_param_values <- c(0.05, 0.1, 0.2, 0.4, 0.8)
  test_zeta_param_values <- c(2, 2.5, 3, 3.5) # Gamma 1.5 omitted as it is makes the mle crash
  test_param_values <- c(test_geom_param_values, test_zeta_param_values)

  # Generate arrays with distribution names to use them in tables
  test_names_geom <- sapply(test_geom_param_values, function(x)
    paste('Geometric [q = ', x,']', sep=''))
  test_names_zeta <- sapply(test_zeta_param_values, function(x)
    paste('Zeta [gamma = ', x,']', sep=''))
  test_names <- c(test_names_geom, test_names_zeta)

  test_filenames_geom <- sapply(test_geom_param_values,
          function(x) paste('test/', test_prefix_geom, x, '.txt', sep=''))
  test_filenames_zeta <- sapply(test_zeta_param_values,
          function(x) paste('test/', test_prefix_zeta, x, '.txt', sep=''))
  test_filenames <- c(test_filenames_geom, test_filenames_zeta)

  # Push the in-degree sequences and theh list of entries to the global scope
  in_degree_sequences <<- sapply(test_filenames, function(x) read.table(x, header=FALSE))
  output_table_labels <<- test_names

  generate_tables(in_degree_sequences)

}

# Function to read the real data and feed it to the pipeline
run_real <- function() {
  # Read the list of languages from file
  languages = read.table("list.txt",
                header = TRUE,
                as.is = c("language","file")
              )

  # Push the in-degree sequences and theh list of entries to the global scope
  in_degree_sequences <<- sapply(languages$file, function(x) read.table(x, header = FALSE))
  output_table_labels <<- languages$language

  generate_tables(in_degree_sequences)
}
```

#Introduction

The goal of this lab session is to practice on information theoretic model selection for the degree distribution of global syntactic dependency networks. We are required to perform a deep analysis over the \textit{in-degree-sequence} dataset. This analysis would be divided in four main phases:

\begin{enumerate}

\item \textbf{Study the basic properties than can be obtained from the given degree sequence}  
As a first step we can directly extract key properties directly from the datasets like their size (\textbf{N}), maximum node degree or mean degree (as it is done in the file \textit{summary\_table.R}). These will be used in the model parameter estimation.

\item \textbf{Estimation of the best parameters for different degree distributions using Maximum Likelihood Estimation (mle)}  
Once we have extracted the basic properties from the sequence we would like to analyze its behavior, i.e. find in an ensemble of degree distribution the one which better fits the real data. To achieve this we make use of R's \textit{mle} method which performs maximum likelihood estimation of parmeters given a log-likelihood function and a set of starting and bound conditions. The parameter values obtained allow to select for each dataset and family of functions which one fits the data best.

\item \textbf{Selection of the best model according to Akaike Information Criterion (AIC)}
After estimating which are the values for the parameters that provide the best fit for each distribution and language we finally have to select which one models best the real data. To do so we use the Akaike Information Criterion (AIC), which provides a measure of how well a model fits the data with a regularization for the amount of parameters. Once the AIC is calculated for all models on a given dataset, we select the one with the lowest score as the best-fitting model. In order to be able to compare the quality of fit between models we use the AIC difference, which is the difference in score between the best-fitting model and the rest. This allows us to see how close are two models on their ability to fit data.
\end{enumerate}

\newpage
\section{Results}

## On code correctness
```{r, echo=FALSE, include=FALSE}
run_test()
```

As suggested in the project statement, before proceeding with the real data we want to make sure our script works properly by running it on datasets generated following known distributions. For that purpose we used the geometric and zeta distribution datasets lecturer, omitting the one for $\gamma=1.5$ as it is a problematic instance that made the _mle_ crash. The table with the model selection obtained (models for which $AIC_{diff}=0$) for each one of the datasets can be found below:

```{r, echo=FALSE}
delta_AIC_df <- data.frame(do.call(rbind, lapply(AICs, function(x) sapply(x, function(y) y - min(x)))), row.names=output_table_labels)
kable(delta_AIC_df,  col.names=models, booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```
We can see that our script correclty selects the distribution used to generate the dataset, choosing between the regular and right-truncated version of the Zeta function for the zeta-generated datasets depending on which one provides the best fit (%EXPLANATION HERE%).  
Now want to check if the parameters of the selected models were properly estimated:
```{r, echo=FALSE}
parameters_df <- data.frame(do.call(rbind, lapply(coef_estimates, unlist)), row.names=output_table_labels)

kable(parameters_df, col.names=params, booktabs=T, linesep='', align='c', digits=3) %>%
  kable_styling(latex_options="striped", full_width=F)
```
By checking the cells corresponding to the parameters of the selected models we can see that the estimation matches the real value quite closely. We can conclude then that the pipeline works properly.

\newpage
## Model selection on the in-degree data
```{r, echo=FALSE, include=FALSE}
run_real()
```

Before starting with the model selection for the real data let's plot the normalized degree spectrum in log-log scalef or all degree sequences:

```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(in_degree_sequences)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(in_degree_sequences), by=1)) {
  spectrum = as.data.frame(table(in_degree_sequences[[i]]))
  plot(x=as.numeric(spectrum$Var1),
       y=as.numeric(spectrum$Freq)/lang_stats[[i]]$N,
       type='o',
       main=output_table_labels[[i]],
       log='xy',
       xlab='k',
       ylab='n(k)/N',
       col='blue')
}
```

As seen in the lectures, the fact that for all sequences the degree spectrum draws an almost straight line in a log-log plot gives us some good intuition that they might follow a power-law, although this linearity gets worse as $k$ increases. This matches with the observation that the in-degree tends to be more power-law than the out-degree, and we can expect the nested Zeta variants and Altmann distributions to be a better fit to the data than the Poisson and geometric ones.

## Degree sequence statistical properties
We can now generate the summary of the properties of the degree sequnces:

```{r, echo=FALSE}
# Apply generate_language_summary to every entry in the list of language in-degrees to generate the summary for every language. Use do.call to pass each summary to rbind as a parameter, that way the vectors are concatenated vertically
summary_df <- data.frame(do.call(rbind, lapply(lang_stats, function(x) c(x$N, x$max, x$M/x$N, x$N/x$M))), row.names=output_table_labels)

kable(summary_df, col.names=c('N', 'Max Degree', 'M/N', 'N/M'), booktabs=T, linesep='', align='c', digits=4) %>%
  kable_styling(latex_options="striped", full_width=F)
```

Although the plots are quite similar to one another, the values of the properties of the different in-degree sequences vary significantly, from the number of nodes to the mean or maximum degree.

## Model parameter estimation
Using the previously calculated parameters and the log-likelihood functions of each model with R's \textit{mle} function we can obtain the parameters which give the best fit for each function and in-degree sequence:

```{r, echo=FALSE}
parameters_df <- data.frame(do.call(rbind, lapply(coef_estimates, unlist)), row.names=output_table_labels)

kable(parameters_df, col.names=params, booktabs=T, linesep='', align='c', digits=3) %>%
  kable_styling(latex_options="striped", full_width=F)
```
It is worth noting that, for each degree sequence, the estimated values of $k_{max}$ both for the right-truncated zeta and Altmann distributions correspond to the maximum degree of the sequence. This behaviour is appropriate given the existing data, as the \textit{mle} finds out that the best value for the maximum degree is that of the maximum degree on the real data. The fact that it's the same for both distributions is also to be expected, as the Altmann distribution is a more general case of the right-truncated zeta (for $\delta=0$), that is, it is another nesting level of the zeta functions.

\newpage
## Model Selection
Using the previous parameter values we can obtain the corresponding log-likelihood value and with it the $AIC_{diff}$ values:

```{r, echo=FALSE}
delta_AIC_df <- data.frame(do.call(rbind, lapply(AICs, function(x) sapply(x, function(y) y - min(x)))), row.names=output_table_labels)
kable(delta_AIC_df,  col.names=models, booktabs=T, linesep='', align='c', digits=2) %>%
  kable_styling(latex_options="striped", full_width=F)
```
We can see how the Altmann function is the better fit for all languages but the Hungarian and Turkish, which seem to be better fitted by a Zeta distribution. It is interesting to note that for Greek, Hungarian, Italian and Turkish the $AIC_{diff}$ values are the same for the Altmann and right-truncated zeta distribution. This happens because, as we mentioned before, the Altmann is a more general case of the right-truncated zeta. If we check the value of $\delta$ on the previous table we can see that for those datasets it equals 0, making the two distributions equal (as the rest of the distributions become identical if $\delta=0$, the estimator gives the same value to the rest of the parameters too).

\newpage
We can now plot the best-fitting model against the original data to be able to visually inspect the quality of the fit:

```{r, echo=FALSE, fig.width=16, fig.height=18}
old.par <- par(mfrow=c(ceiling(length(in_degree_sequences)/3), 3))
par(mar=c(4,4,2,2))
for (i in seq(from=1, to=length(in_degree_sequences), by=1)) {
  best <- as.numeric(which.min(delta_AIC_df[i,]))
  best_params <- as.list(coef_estimates[[i]][[best]])
  spectrum <- as.data.frame(table(in_degree_sequences[[i]]))
  best_fit <- sapply(as.numeric(spectrum$Var1), function(x) do.call(pmfs[[best]], c(x, best_params)))
  plot(x=as.numeric(spectrum$Var1),
       y=as.numeric(spectrum$Freq)/lang_stats[[i]]$N,
       type='o',
       main=paste(output_table_labels[[i]], ' - Best fit: ', models[best], sep=''),
       log='xy',
       xlab='k',
       ylab='n(k)/N',
       col='blue')

  lines(x=as.numeric(spectrum$Var1),
       y=best_fit,
       type='o',
       col='red')

  legend("topright", legend=c("Real data", models[best]),
       col=c('blue', 'red'), lty=1, pch=1, cex=1.8)
}
```
The fit seems to be quite good for all functions, Italian and Greek are tied

\newpage
\section{Discussion}

By observing the quality of the fit for the Altmann and Zeta functions as well as the $AIC_{diff}$ values obtained we can conclude that the in-degree distribution of the languages studied is significantly more similar to a a power-law than to any of the null models (Poisson and geometric) considered (which are specially poor. which are not very good models of the data. One possible reason for this is the nature itself of null models, which are meant to match a given object in some of its features but are otherwise consiedered to be a random structure. Because language is not random its syntactic dependencies won't be either, and although the Null model might be able to capture some aspect of the in-degree distribution it may not be able to represent it with the same success as other, more structured models. 

As we saw earlier, the Altmann distribution is the best fit in most cases, dominating over the other distributions in 8/10 cases. As we also saw before, this happens because the Altmann is a possible next nesting level of the right-truncated zeta function, a more general version of it. Because of this, in any situation in which the right-truncated zeta would be the best model automatically the Altmann function is at least as a good fit as it (because taking $\delta=0$ makes them identical), with the addition of having an extra parameter that allows for some fine-tuning (Notice that for Italian and Greek both AIC scores are tied). Inspecting the plots, we can see that most of the languages keep a somewhat regular slope for large values of $k$ despite the degradation of the linearity, so the Altmann function would be a pretty good fit for them as this linearity allows to roughly locate where the maximum degree value would be. For Hungarian and Turkish we can see how the slope diminishes at the end, having nodes further away than one intially would expect and making the Zeta function (which can be obtained from the truncated zeta by making $k_{max}$ go to infinity) a better fit than the truncated ones. Nevertheless, whenever the Zeta is the best fit the truncated versions follow quite closely in terms of $AIC_{diff}$ score.

% - The extent to which languages resemble or differ.



% - Conclusions

\newpage
\section{Methods}
In order to minimize the amount of repeated code and to enable an easy way to add more functions without having to modify the main body of the script, for this project we have implemented our solution using functional programming. This has allowed us to define a pipeline which takes a list of probability mass functions, minus log likelihoods, parameters and bounds (which can be read from file and easily expanded) and performs the fitting and model selection, returning the data structures necesary to generate the tables and the plots.

For our additional, better fitting distribution we have used a modified version of the Altmann function, as proposed in the project statement and originally by G. Altmann as a mathematical statement of the Menzerath-Altmann law. The difference between our version and the one in the statement lies in using a third parameter $k_max$ instead of $N$ to calculate $c$ in the same spirit as the Right-Truncated Zeta distribution. This way the optimizer can (and will) adjust $k_{max}$ accordingly to provide a better value for $c$. The Altmann function has the form:
\[p(k)=ck^{-\gamma}e^{-\delta k}\]
for $1 \leq k \leq N$ and 0 otherwise, $c$ being:
\[\frac{1}{\sum^{k_{max}}_{k=1} k^{-\gamma}e^{-\delta k} } \]

Its log-likelihood can be derived as follows:
\[ \sum^N_{k=1} log(ck^{-\gamma}e^{-\delta k}) = \sum^N_{k=1} log(c) -\gamma \cdot log(k) -\delta \cdot k = N \cdot log(c) - \gamma \cdot M' -\delta \cdot M \]

Regarding to the starting values and bounds for the mle we have tried to stick to the ones proposed in the statement as much as possible. As such, we selected for our starting values $\lambda=M/N$ (Poisson), $q=N/M$ (Geometric) and $\gamma=2$ (Zeta & Altmann distributions), with $\lambda > 1$, $0 < q < 1$ and $\gamma > 1$. For $k_{max}$ (both in the right-truncated zeta & Altmann) we gave it an inital value equal to the maximum degree in the list. In order for the optimizer to select that value we . This allows the optimizer to select the maximum degree value for $k_{max}$, which is the best estimation possible (the real value). Because the mle crashes without an upper bound for $k_{max}$, we set it to twice the maximum degree in order to provide some space for the optimizer to perform the search. Finally, for the Altmann function's $\delta$ we made it so $0 < \delta < 1$ and gave it an initial value of 0.5 after a fair amount of trial and error.

\section*{Appendix}

Akaike Information Criterion for small samples:

\begin{equation}\label{eq:aic}
    AIC_c = -2\mathcal{L} + 2K\frac{N}{N-K-1}
\end{equation}
